# AdaBoost

自适应增强提升算法
一般是二分类算法

>AdaBoost全称为AdaptiveBoosting:自适应提升算法；虽然名字听起来给人一种高大上的感觉，但其实背后的原理并不难理解。什么叫做自适应，就是这个算法可以在不同的数据集上都适用,这个基本和废话一样,一个算法肯定要能适应不同的数据集。

提升方法是指:分类问题中，通过改变训练样本的权重，学习多个分类器，并将这些分类器进行线性组合，提高分类器的性能。
利用前一轮迭代弱学习器的误差率来更新训练集的权重，这样一轮轮的迭代下去

>但是标准的adaboost只适用于二分类任务。

## boosting过程

Boosting分类方法，其过程如下所示：

1.先通过对N个训练数据的学习得到第一个弱分类器h1；

2.将h1分错的数据和其他的新数据一起构成一个新的有N个训练数据的样本，通过对这个样本的学习得到第二个弱分类器h2；

3.将h1和h2都分错了的数据加上其他的新数据构成另一个新的有N个训练数据的样本，通过对这个样本的学习得到第三个弱分类器h3；

4.最终经过提升的强分类器h_final=Majority Vote(h1,h2,h3)。即某个数据被分为哪一类要通过h1,h2,h3的**多数表决**。
上述Boosting算法，存在两个问题：

**如何调整训练集，使得在训练集上训练弱分类器得以进行**。
**如何将训练得到的各个弱分类器联合起来形成强分类器**。

针对以上两个问题，AdaBoost算法进行了调整：

1.使用加权后选取的训练数据代替随机选取的训练数据，这样将**训练的焦点集中在比较难分的训练数据上**。

2.将弱分类器联合起来时，使用**加权的投票机制代替平均投票机制**。让**分类效果好的弱分类器具有较大的权重**，而**分类效果差的分类器具有较小的权重**。

这个很好理解:smile:

## 推导
[【参考知乎：一文弄懂AdaBoost】](https://zhuanlan.zhihu.com/p/59751960)

**训练数据**：$T=\left\{\left(x_{1}, y_{1}\right),\left(x_{2}, y_{2}\right), \ldots\left(x_{n}, y_{n}\right)\right\}$，其中$x_{i} \in \chi \subseteq R^{n}$，$y \in Y\{-1,+1\}$，最后需要得到分类器：$G(x)=\sum_{m=1}^{M} a_{m} G_{m}(x)$，其中 $m $为分类器的个数，每一次训练我们都获得一个基分类器 $G_{i}(x)$,$a_i$ 是每个基训练器的权重，也就是说每个基分类器说话的分量。我们看最后的分类器，他就是结合多个不同基分类器的意见，集百家之长，最终输出结果。

## 权重

Adaboost训练过程中有两个权重，一个是怎加被分错训练样本的权重，另一个就是增加误差比较小的弱分类器的权重。
针对为何要提高被分错样本的权重，可以参考[cnblog](https://www.cnblogs.com/pinard/p/6133937.html)

个人感觉就是，Adaboost的最终目的是给误差较小的弱分类器的赋加大的权重基于此构造一个强分类器，但在训练基分类器的过程中会增加没被分类正确的样本的权重，使其在下一轮训练中继续分，若分对了则总的loss会减小，此时进行下一轮弱分类器的训练。

这样总结一下思路会比较清晰就是很罗嗦，嘿嘿，表达有待提高。。


## 加法模型

就是增加一个前一轮的弱分类器已经变成了强分类器，那在本轮训练中只考虑当前训练的弱分类器能否使loss减小即可。


也就是一个函数（模型）是由**多个函数（模型）累加**起来的$f(x)=\sum_{m=1}^{M} \beta_{m} b\left(x ; \gamma_{m}\right)$

其中 $\beta_{m}$是每个基函数的系数， $\gamma_{m}$ 是每个基函数的参数， $b\left(x ; \gamma_{m}\right)$ 就是一个基函数了

假设一个基函数为 $e^{ax}$ ，那么一个加法模型就可以写成: $f(x)=e^{x}+2 e^{2 x}-2 e^{x / 2}$

## 前向分步算法

在给定训练数据以及损失函数 $L(y,f(x))$ 的情况下，加法模型的经验风险最小化即损失函数极小化问题如下:
$\min _{\beta_{m}, \gamma_{m}} \sum_{i=1}^{N} L\left(y_{i}, \sum_{m=1}^{N} \beta_{m} b\left(x ; \gamma_{m}\right)\right)$

这个问题直接优化比较困难，前向分步算法解决这个问题的思想如下:由于我们最终的分类器其实加法模型，所以我们可以从前向后考虑，**每增加一个基分类器，就使损失函数$L(y,f(x))$的值更小一点，逐步的逼近最优解**。这样考虑的话，**每一次计算损失函数的时候，我们只需要考虑当前基分类器的系数和参数**，同时**此次之前基分类器的系数和参数不受此次的影响**。算法的思想有点类似梯度下降，**每一次都向最优解移动一点**

步骤

**输入训练数据**：$T=\left\{\left(x_{1}, y_{1}\right),\left(x_{2}, y_{2}\right), \ldots\left(x_{n}, y_{n}\right)\right\}$，其中$x_{i} \in \chi \subseteq R^{n}$，$y \in Y\{-1,+1\}$，最后需要得到分类器$G(X)$

**初始化训练值的权值分布**

$D_{1}=\left(w_{1 i}, w_{2 i}, \ldots, w_{1 N}\right) ，w_{1 i}=\frac{1}{N}$

对于$m=1,2,...,M$
**a**使用具有权值分布 $D_{m} 的训练数据集学习，得到基本分类器$G_{m}(x)$ 。

**b**计算 $G_{m}(x)$在训练集上的分类误差率

$e_{m}=\sum_{i=1}^{N} w_{m i} I\left\{y_{i} \neq G_{m}\left(x_{i}\right)\right\}$

**c**计算 $G_{m}(x)$的系数
$\alpha_{m}=\frac{1}{2} \log \frac{1-e_{m}}{e_{m}}$

**d**根据前m-1 次得到的结果，更新权值:
$w_{m=1, i}=\frac{w_{m i} e^{-y_{i} \alpha_{m} G_{m}\left(x_{i}\right)}}{Z_{m}}$

其中 $Z_{m}=\sum_{i=1}^{N} w_{m i} e^{-y_{i} \alpha_{m} G_{m}\left(x_{i}\right)}$,是一个规范化因子，用于归一化

**构建最终的分类器**
$f(x)=\sum_{m=1}^{M} a_{m} G_{m}(x)$
$G(x)=\operatorname{sign}(f(x))$

## Adaboost的数据权重

>如果训练数据保持不变，那么在数据的某个特定维度上单层决策树找到的最佳决策点每一次必然都是一样的，为什么呢？因为单层决策树是把所有可能的决策点都找了一遍然后选择了最好的，如果训练数据不变，那么每次找到的最好的点当然都是同一个点了。
>所以，这里Adaboost数据权重就派上用场了，所谓“数据的权重主要用于弱分类器寻找其分类误差最小的点”，其实，在单层决策树计算误差时，Adaboost要求其乘上权重，即计算带权重的误差。

举个例子，在以前没有权重时（其实是平局权重时），一共10个点时，对应每个点的权重都是0.1，分错1个，错误率就加0.1；分错3个，错误率就是0.3。现在，每个点的权重不一样了，还是10个点，权重依次是[0.01,0.01,0.01,0.01,0.01,0.01, 0.01,0.01,0.01,0.91]，如果分错了第1一个点，那么错误率是0.01，如果分错了第3个点，那么错误率是0.01，要是分错了最后一个点，那么错误率就是0.91。这样，在**选择决策点的时候自然是要尽量把权重大的点（本例中是最后一个点）分对才能降低误差率**。由此可见，权重分布影响着单层决策树决策点的选择，权重大的点得到更多的关注，权重小的点得到更少的关注。

>在Adaboost算法中，每训练完一个弱分类器都就会调整权重，上一轮训练中被误分类的点的权重会增加，在本轮训练中，由于权重影响，本轮的弱分类器将更有可能把上一轮的误分类点分对，如果还是没有分对，那么分错的点的权重将继续增加，下一个弱分类器将更加关注这个点，尽量将其分对。

**这一点也说明Adaboost对异常值会很敏感，因此对剔除异常值的需求比较大**

这样，达到“你分不对的我来分”，下一个分类器主要关注上一个分类器没分对的点，每个分类器都各有侧重。

## Adaboost分类器的权重

由于Adaboost中若干个分类器的关系是第N个分类器更可能分对第N-1个分类器没分对的数据，而不能保证以前分对的数据也能同时分对。所以在Adaboost中，每个弱分类器都有各自最关注的点，每个弱分类器都只关注整个数据集的中一部分数据，所以它们必然是共同组合在一起才能发挥出作用。所以最终投票表决时，需要根据弱分类器的权重来进行加权投票，权重大小是根据弱分类器的分类错误率计算得出的，总的规律就是弱分类器错误率越低，其权重就越高。[原理](http://www.uml.org.cn/sjjmwj/2019030721.asp)


## 优劣
Adaboost的主要优点有：

- Adaboost作为分类器时，分类精度很高
- 在Adaboost的框架下，可以使用各种回归分类模型来构建弱学习器，非常灵活。
- 作为简单的二元分类器时，构造简单，结果可理解。
- 不容易发生过拟合
- Adaboost的主要缺点有：
- 对异常样本敏感，异常样本在迭代中可能会获得较高的权重，影响最终的强学习器的预测准确性。

Adaboost有分类树和回归树两种。回归树就是以CART作为基分类器

## boosting与adaboost的关系

提升树和AdaBoost之间的关系就好像编程语言中对象和类的关系，一个类可以生成多个不同的对象。提升树就是AdaBoost算法中基分类器选取决策树桩得到的算法。

用于分类的决策树主要有利用ID3和C4.5两种算法，我们选取任意一种算法，生成只有一层的决策树，即为决策树桩。

## 残差树

我们可以看到AdaBoost和提升树都是针对分类问题，如果是回归问题，上面的方法就不奏效了；而残差树则是针对回归问题的一种提升方法。其基学习器是基于CART算法的回归树，模型依旧为加法模型、损失函数为平方函数、学习算法为前向分步算法。

## 复现

参数
**base_estimator：**基分类器，默认是决策树，在该分类器基础上进行boosting，理论上可以是任意一个分类器，但是如果是其他分类器时需要指明样本权重

**n_estimators:**基分类器提升（循环）次数，默认是50次，这个值过大，模型容易过拟合；值过小，模型容易欠拟合。

**learning_rate:**学习率，表示梯度收敛速度，默认为1，如果过大，容易错过最优值，如果过小，则收敛速度会很慢；该值需要和n_estimators进行一个权衡，当分类器迭代次数较少时，学习率可以小一些，当迭代次数较多时，学习率可以适当放大。

**algorithm:boosting**算法，也就是模型提升准则，有两种方式SAMME, 和SAMME.R两种，默认是SAMME.R，两者的区别主要是弱学习器权重的度量，前者是对样本集预测错误的概率进行划分的，后者是对样本集的预测错误的比例，即错分率进行划分的，默认是用的SAMME.R。

**random_state:**随机种子设置。

### 属性
**estimators_:**以列表的形式返回所有的分类器。

**classes_:**类别标签

**estimator_weights_:**每个分类器权重

**estimator_errors_:**每个分类器的错分率，与分类器权重相对应。

**feature_importances_:**特征重要性，这个参数使用前提是基分类器也支持这个属性。

>关于Adaboost模型本身的参数并不多，但是我们在实际中除了调整Adaboost模型参数外，还可以调整基分类器的参数，关于基分类的调参，和单模型的调参是完全一样的，比如默认的基分类器是决策树，那么这个分类器的调参和我们之前的Sklearn参数详解——决策树是完全一致。

方法
decision_function(X):返回决策函数值（比如svm中的决策距离）

fit(X,Y):在数据集（X,Y）上训练模型。

get_parms():获取模型参数

**predict(X):**预测数据集X的结果。

predict_log_proba(X):预测数据集X的对数概率。

**predict_proba(X)**:预测数据集X的概率值。

score(X,Y):输出数据集（X,Y）在模型上的准确率。

staged_decision_function(X):返回每个基分类器的决策函数值

staged_predict(X):返回每个基分类器的预测数据集X的结果。

staged_predict_proba(X):返回每个基分类器的预测数据集X的概率结果。

staged_score(X, Y):返回每个基分类器的预测准确率。

datacamp的栗子

```python
# Import DecisionTreeClassifier
from sklearn.tree import DecisionTreeClassifier

# Import AdaBoostClassifier
from sklearn.ensemble import AdaBoostClassifier

# Instantiate dt
dt = DecisionTreeClassifier(max_depth=2, random_state=1)

# Instantiate ada
ada = AdaBoostClassifier(base_estimator=dt, n_estimators=180, random_state=1)

# Fit ada to the training set
ada.fit(X_train, y_train)

# Compute the probabilities of obtaining the positive class
y_pred_proba = ada.predict_proba(X_test)[:,1]

# Import roc_auc_score
from sklearn.metrics import roc_auc_score

# Evaluate test-set roc_auc_score
ada_roc_auc = roc_auc_score(y_test, y_pred_proba)

# Print roc_auc_score
print('ROC AUC score: {:.2f}'.format(ada_roc_auc))

<script.py> output:
    ROC AUC score: 0.71
```
