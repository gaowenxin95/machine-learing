# XGBoost

## 定义

a novel sparsity-aware algorithm for sparse data and weighted quantile sketch for approximate tree learning. [@DBLP:journals/corr/ChenG16]

>provide insights on cache access patterns, data compression and sharding to build a scalable tree boosting system. By combining these insights, XGBoost scales beyond billions of examples using far fewer resources than existing systems

- 提供了一种有关缓存访问模式，数据压缩和分片，以构建可扩展的boosting tree。 因此，XGBoost使用比现有系统少得多的资源来扩展数十亿个示例。

## 特点

>The scalability of XGBoost is due to several important systems and algorithmic optimizations. These innovations include: a novel tree learning algorithm is for handling sparse data; a theoretically justified weighted quantile sketch procedure enables handling instance weights in approximate tree learning. Parallel and distributed computing makes learning faster which enables quicker model exploration. More importantly, XGBoost exploits out-of-core computation and enables data scientists to process hundred  millions of examples on a desktop. Finally, it is even more exciting to combine these techniques to make an end-to-end system that scales to even larger data with the least amount of cluster resources.

- 可以处理稀疏矩阵
- 合适的权重使得误差较小
- 并行计算更快
- 可以利用核外资源，更快的处理大规模的数据
- 可扩展性强
- 构成一个端到端系统，以最少的集群资源扩展到更大的数据

### 回归树综述

**回归树也叫回归与分类树，也是一棵叶子结点具有权重的二叉树**

- 分类的规则与决策树的规则一样

- 每个叶子结点都包含一个权重（分数）

- 优点

  - 对数据输入范围不敏感，因此不需要归一化处理
  - 能学习特征之间更高阶别的相互关系
  - 可拓展性强
  - 适用范围广

## 正则化目标函数

假设一个数据集有$n$个样本和$m$个特征，$\mathcal{D}=\left\{\left(\mathbf{x}_{i}, y_{i}\right)\right\}\left(|\mathcal{D}|=n, \mathbf{x}_{i} \in \mathbb{R}^{m}, y_{i} \in \mathbb{R}\right)$，一个集成树使用$k$个加法模型进行预测。

$\hat{y}_{i}=\sum_{k=1}^{K} f_{k}\left(x_{i}\right), f_{k} \in \mathcal{F}$      (1)

- $\hat{y}_{i}$表示第 i个样本的预测值
- K 表示一共K棵树
- 其中$\mathcal{F}=\left\{f(\mathbf{x})=w_{q(\mathbf{x})}\right\}\left(q: \mathbb{R}^{m} \rightarrow T, w \in \mathbb{R}^{T}\right)$ ,$\mathcal{F}$表示所有基分类器组成的函数空间，比如CART.
- $q$表示表示将样本映射到的每棵树上相应的叶子索引
- $T$是叶子结点的数量
- $f_{k}(x_i)$表示第i个样本样本第k棵树中落在叶子结点得到的权重值

假设我们boosting的基模型用决策树来实现，则一颗生成好的决策树，即结构确定，也就是说树的叶子结点其实是确定了的。假设这棵树的叶子结点有 $T$ 片叶子，而每片叶子对应的值 $w \in R^{T}$ 。熟悉决策树的同学应该清楚，每一片叶子结点中样本的预测值都会是一样的，在**分类问题中是某一类**，**在回归问题中，是某一个值**（在GBDT中都是回归树，即分类问题转化成对概率的回归了），那么肯定存在这样一个函数$q:R^d->{1,2,...T}$,即将 $f_{t}(x)$ 中的每个样本映射到每一个叶子结点上，当然 $f_{t}(x)$和 q 我们都是不知道的，但我们也不关心，这里只是说明一下决策树表达数据结构的方法是怎么样的，不理解也没有问题。

下面来正式推导：

$f_{t}(x)$可以转化为$w_{q(x)}$,其中$q(x)$ 代表了每个样本在哪个叶子结点上,而 $w_q$ 则代表了哪个叶子结点取什么 $w$ 值，所以 $w_{q(x)}$ 就代表了每个样本的取值$w$ (即预测值).

如果决策树的复杂度可以由正则项来定义 $\Omega\left(f_{t}\right)=\gamma T+\frac{1}{2} \lambda \sum_{j=1}^{T} w_{j}^{2}$ ，即决策树模型的复杂度由生成的树的叶子节点数量和叶子节点对应的值向量的L2范数决定。

**在回归问题中，需要求出的就是$f_{k}$,也就是每个树的结构和每个叶子结点的权重**

## XGBoost目标函数

$\mathcal{L}(\phi)=\sum_{i} l\left(\hat{y}_{i}, y_{i}\right)+\sum_{k} \Omega\left(f_{k}\right)$

where $\Omega(f)=\gamma T+\frac{1}{2} \lambda\|w\|^{2}$     (2)

- $\sum_{i} l\left(\hat{y}_{i},y_{i}\right)$是损失函数，也就是训练误差，换句话说就是训练出来的函数与测试集的匹配程度。
- $\sum_{k} \Omega\left(f_{k}\right)$表示模型的复杂度，也就是分段的琐碎程度
- 加入正则惩罚项时为了防止过拟合

**为什么说加入正则化可以防止过拟合**
  正则项时我们假设反应模型的复杂度的，因此模型应该是越简单越好，需要加入正则项进行控制
- L2范数Ridge：$\Omega(w)=\lambda\|w\|^{2}$，可以认为是摸的平方
- L1范数Lasso：$\Omega(w)=\lambda\|w\|_{1}$

### 常见的正则优化函数

- 岭回归Ridge：由方差和L2范数构成
$\sum_{i=1}^{n}\left(y_{i}-w^{T} x_{i}\right)^{2}+\lambda\|w\|^{2}$

- 套索回归Lasso：方差和L1范数构成
$\sum_{i=1}^{n}\left(y_{i}-w^{T} x_{i}\right)^{2}+\lambda\|w\|_{1}$

- logistics回归：logistics误差和L2范数构成，主要用于二分类问题，最常见
$\sum_{i=1}^{n}\left[y_{i} \ln \left(1+e^{-w^{T} x_{i}}\right)+\left(1-y_{i}\right) \ln \left(1+e^{w^{T} x_{i}}\right)\right]+\lambda\|w\|^{2}$

训练模型的目标就是得到最小化的损失函数，因此需要，$\mathcal{L}(\phi)$越小越好，那就是$\sum_{i} l\left(\hat{y}_{i},y_{i}\right)$和正则项都取小，但是根据trade-off思维，需要权衡bias和variance的大小



### 损失函数分类

- 若训练误差为：$l\left(y, \hat{y}_{i}\right)=\left(y_{i}-\hat{y}_{i}\right)^{2}$，此使就是GBDT
- 若训练误差为$l\left(y, \hat{y}_{i}\right)=y_{i} \ln \left(1+e^{-\hat{y}_{i}}+\left(1-y_{i}\right) \ln \left(1+e^{\hat{y}_{i}}\right)\right)$那就是logistics

## 贪心算法

贪心算法的思想就是一棵树，一棵树的往上加，加到K棵树直到算法停止
对于Boosting来说，它采用的是前向优化算法，即从前往后，逐渐建立基模型来优化逼近目标函数，具体过程如下：

$$
\begin{aligned}
&\hat{y}_{i}^{0}=0\\
&\hat{y}_{i}^{1}=f_{1}\left(x_{i}\right)=\hat{y}_{i}^{0}+f_{1}\left(x_{i}\right)\\
&\begin{array}{l}
\hat{y}_{i}^{2}=f_{1}\left(x_{i}\right)+f_{2}\left(x_{i}\right)=\hat{y}_{i}^{1}+f_{2}\left(x_{i}\right) \\
\therefore \quad \hat{y}_{i}^{t}=\sum_{k=1}^{t} f_{k}\left(x_{i}\right)=\hat{y}_{i}^{t-1}+f_{t}\left(x_{i}\right)
\end{array}
\end{aligned}
$$

其中$f_{k}$的加入，也就是新模型的加入总是以优化目标函数为目的的。

### 过程

以第t步的模型拟合为例，在这一步，模型对第 $i$个样本 $x_i$ 的预测为：
$\hat{y}_{i}^{t}=\hat{y}_{i}^{t-1}+f_{t}\left(x_{i}\right)$


其中 $f_{t}(x_{i})$ 就是我们这次需要加入的新模型，即需要拟合的模型，此时，目标函数就可以写成：

$\begin{aligned} O b j^{(t)} &=\sum_{i=1}^{n} l\left(y_{i}, \hat{y}_{i}^{t}\right)+\sum_{i=i}^{t} \Omega\left(f_{i}\right) \\ &=\sum_{i=1}^{n} l\left(y_{i}, \hat{y}_{i}^{t-1}+f_{t}\left(x_{i}\right)\right)+\Omega\left(f_{t}\right)+\text { constant } \end{aligned}$             (3)

因此当求出最优目标函数的时候也就相当于求出了$f_{t}\left(x_{i}\right)$ 



我们知道泰勒公式中，若$\Delta x$ 很小时，我们只保留二阶导是合理的（GBDT是一阶导，XGBoost是二阶导，我们以二阶导为例，一阶导可以自己去推，因为更简单）
**或许也可以说我们更希望将优化问题转化为一个凸优化问题，因此而引入二阶泰特展开式**，即：

$f(x+\Delta x) \approx f(x)+f^{\prime}(x) \Delta x+\frac{1}{2} f^{\prime \prime}(x) \Delta x^{2}$ (4)  

那么在等式（3）中，我们把 $\hat{y}_{i}^{t-1}$ 看成是等式（4）中的x， $f_{t}\left(x_{i}\right)$  看成是 $\Delta x$ ，因此等式（3）可以写成：

$O b j^{(t)}=\sum_{i=1}^{n}\left[l\left(y_{i}, \hat{y}_{i}^{t-1}\right)+g_{i} f_{t}\left(x_{i}\right)+\frac{1}{2} h_{i} f_{t}^{2}\left(x_{i}\right)\right]+\Omega\left(f_{t}\right)+$ constant  (5)

其中 $g_{i}$ 为损失函数的一阶导， $h_i$ 为损失函数的二阶导，注意这里的导是对 $\hat{y}_{i}^{t-1}$ 求导。我们以 平方损失函数为例$\sum_{i=1}^{n}\left(y_{i}-\left(\hat{y}_{i}^{t-1}+f_{t}\left(x_{i}\right)\right)\right)^{2}$ ，则分别给出$g_i$,$h_i$

$g_{i}=\partial_{\hat{y}^{t-1}}\left(\hat{y}^{t-1}-y_{i}\right)^{2}=2\left(\hat{y}^{t-1}-y_{i}\right), 
\quad h_{i}=\partial_{\hat{y}^{t-1}}^{2}\left(\hat{y}^{t-1}-y_{i}\right)^{2}=2$

由于在第t步 $\hat{y}_{i}^{t-1}$ 其实是一个已知的值，所以 $l\left(y_{i}, \hat{y}_{i}^{t-1}\right)$ 是一个常数，其对函数优化不会产生影响，因此，等式（3）可以写成：
$O b j^{(t)} \approx \sum_{i=1}^{n}\left[g_{i} f_{t}\left(x_{i}\right)+\frac{1}{2} h_{i} f_{t}^{2}\left(x_{i}\right)\right]+\Omega\left(f_{t}\right)$    (6)



所以我么只要求出每一步损失函数的一阶和二阶导的值（由于前一步的 $\hat{y}_{i}^{t-1}$ 是已知的，所以这两个值就是常数）代入等式4，然后最优化目标函数，就可以得到每一步的 $f(x)$ ，最后根据加法模型得到一个整体模型

### 如何使用决策树表示目标函数

假设我们boosting的基模型用决策树来实现，则一颗生成好的决策树，即结构确定，也就是说树的叶子结点其实是确定了的。假设这棵树的叶子结点有 $T$ 片叶子，而每片叶子对应的值 $w \in R^{T}$ 。熟悉决策树的同学应该清楚，每一片叶子结点中样本的预测值都会是一样的，在**分类问题中是某一类**，**在回归问题中，是某一个值**（在GBDT中都是回归树，即分类问题转化成对概率的回归了），那么肯定存在这样一个函数$q:R^d->{1,2,...T}$,即将 $f_{t}(x)$ 中的每个样本映射到每一个叶子结点上，当然 $f_{t}(x)$和 q 我们都是不知道的，但我们也不关心，这里只是说明一下决策树表达数据结构的方法是怎么样的，不理解也没有问题。

下面来正式推导：

$f_{t}(x)$可以转化为$w_{q(x)}$,其中$q(x)$ 代表了每个样本在哪个叶子结点上,而 $w_q$ 则代表了哪个叶子结点取什么 $w$ 值，所以 $w_{q(x)}$ 就代表了每个样本的取值$w$ (即预测值).

如果决策树的复杂度可以由正则项来定义 $\Omega\left(f_{t}\right)=\gamma T+\frac{1}{2} \lambda \sum_{j=1}^{T} w_{j}^{2}$ ，即决策树模型的复杂度由生成的树的叶子节点数量和叶子节点对应的值向量的L2范数决定。

我们假设 $I_{j}=\left\{i | q\left(x_{i}\right)=j\right\}$ 为第 j 个叶子节点的样本集合，则等式4根据上面的一些变换可以写成：

$\begin{aligned} O b j^{(t)} & \approx \sum_{i=1}^{n}\left[g_{i} f_{t}\left(x_{i}\right)+\frac{1}{2} h_{i} f_{t}^{2}\left(x_{i}\right)\right]+\Omega\left(f_{t}\right) \\ &=\sum_{i=1}^{n}\left[g_{i} w_{q\left(x_{i}\right)}+\frac{1}{2} h_{i} w_{q\left(x_{i}\right)}^{2}\right]+\gamma T+\frac{1}{2} \lambda \sum_{j=1}^{T} w_{j}^{2} \\ &=\sum_{j=1}^{T}\left[\left(\sum_{i \in I_{j}} g_{i}\right) w_{j}+\frac{1}{2}\left(\sum_{i \in I_{j}} h_{i}+\lambda\right) w_{j}^{2}\right]+\gamma T \end{aligned}$                                           (7)

即我们之前样本的集合，现在都改写成叶子结点的集合，由于一个叶子结点有多个样本存在，因此才有了$\sum_{i \in I_{j}} g_{i}$和$\sum_{i \in I_{j}} h_{i}$两 项，定义$G_i=\sum_{i \in I_{j}} g_{i}$,$H_i=\sum_{i \in I_{j}} h_{i}$,则公式7可以写成:
$O b j^{(t)}=\sum_{j=1}^{T}\left[G_{j} w_{j}+\frac{1}{2}\left(H_{j}+\lambda\right) w_{j}^{2}\right]+\gamma T$

### 如何优化目标函数

那么对于单棵决策树，一种理想的优化状态就是枚举所有可能的树结构，因此过程如下：

a、首先枚举所有可能的树结构，即  $q$；

b、计算每种树结构下的目标函数值，即等式7的值；

c、取目标函数最小（大）值为最佳的数结构，根据等式6求得每个叶子节点的 $w$ 取值，即样本的预测值。

但上面的方法肯定是不可行的，因为树的结构千千万，所以一般用贪心策略来优化：

a、从深度为0的树开始，对每个叶节点枚举所有的可用特征

b、 针对每个特征，把属于该节点的训练样本根据该特征值升序排列，通过线性扫描的方式来决定该特征的最佳分裂点，并记录该特征的最大收益（采用最佳分裂点时的收益）

c、 选择收益最大的特征作为分裂特征，用该特征的最佳分裂点作为分裂位置，把该节点生长出左右两个新的叶节点，并为每个新节点关联对应的样本集

d、回到第1步，递归执行到满足特定条件为止

### 如何分裂一个结点

那么如何计算上面的收益呢，很简单，仍然紧扣目标函数就可以了。假设我们在某一节点上二分裂成两个节点，分别是左（L）右（R），则分列前的目标函数是:
$$-\frac{1}{2}\left[\frac{\left(G_{L}+G_{R}\right)^{2}}{H_{L}+H_{R}+\lambda}\right]+\gamma$$
分裂后$$-\frac{1}{2}\left[\frac{G_{L}^{2}}{H_{L}+\lambda}+\frac{G_{R}^{2}}{H_{R}+\lambda}\right]+2\gamma$$，则对于目标函数来说，分裂后的收益是（这里假设是最小化目标函数，所以用分裂前-分裂后）
$$Gain =\frac{1}{2}\left[\frac{G_{L}^{2}}{H_{L}+\lambda}+\frac{G_{R}^{2}}{H_{R}+\lambda}-\frac{\left(G_{L}+G_{R}\right)^{2}}{H_{L}+H_{R}+\lambda}\right]-\gamma$$

如果增益Gain>0，即分裂为两个叶子节点后，目标函数下降了，那么我们会考虑此次分裂的结果。

但是，在一个结点分裂时，可能有很多个分裂点，每个分裂点都会产生一个增益，如何才能寻找到最优的分裂点呢？接下来会讲到。

### 寻找最佳分裂点
在分裂一个结点时，我们会有很多个候选分割点，寻找最佳分割点的大致步骤如下：

遍历每个结点的每个特征；
对每个特征，按特征值大小将特征值排序；
线性扫描，找出每个特征的最佳分裂特征值；
在所有特征中找出最好的分裂点（分裂后增益最大的特征及特征值）
上面是一种贪心的方法，每次进行分裂尝试都要遍历一遍全部候选分割点，也叫做全局扫描法。

但当数据量过大导致内存无法一次载入或者在分布式情况下，贪心算法的效率就会变得很低，全局扫描法不再适用。

- 特征预排序+缓存：XGBoost在训练之前，预先对每个特征按照特征值大小进行排序，然后保存为block结构，后面的迭代中会重复地使用这个结构，使计算量大大减小。

- 分位点近似法：对每个特征按照特征值排序后，采用类似分位点选取的方式，仅仅选出常数个特征值作为该特征的候选分割点，在寻找该特征的最佳分割点时，从候选分割点中选出最优的一个。

- 并行查找：由于各个特性已预先存储为block结构，XGBoost支持利用多个线程并行地计算每个特征的最佳分割点，这不仅大大提升了结点的分裂速度，也极利于大规模训练集的适应性扩展。

### 停止生长
一棵树不会一直生长下去，下面是一些常见的限制条件。

(1) 当新引入的一次分裂所带来的增益Gain<0时，放弃当前的分裂。这是训练损失和模型结构复杂度的博弈过程。


(2) 当树达到最大深度时，停止建树，因为树的深度太深容易出现过拟合，这里需要设置一个超参数max_depth。

(3) 当引入一次分裂后，重新计算新生成的左、右两个叶子结点的样本权重和。如果任一个叶子结点的样本权重低于某一个阈值，也会放弃此次分裂。这涉及到一个超参数:最小样本权重和，是指如果一个叶子节点包含的样本数量太少也会放弃分裂，防止树分的太细，这也是过拟合的一种措施。

## 总结
a、算法在拟合的每一步都新生成一颗决策树；

b、在拟合这棵树之前，需要计算损失函数在每个样本上的一阶导和二阶导，即 $g_i$ 和 $h_i$ ；

c、通过上面的贪心策略生成一颗树，计算每个叶子结点的的 $G_j$和 $H_j$ ，利用等式6计算预测值 $w$ ；

d、把新生成的决策树 $f_{t}(x)$ 加入 $\hat{y}_{i}^{t}=\hat{y}_{i}^{t-1}+\epsilon f_{t}\left(x_{i}\right)$ ，其中$\epsilon$ 为学习率，主要为了抑制模型的过拟


