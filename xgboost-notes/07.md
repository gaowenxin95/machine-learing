# 特征选择

## 计算特征重要性

>使用梯度提升算法的好处是在提升树被创建后，可以相对直接地得到每个属性的重要性得分。一般来说，重要性分数，衡量了特征在模型中的提升决策树构建中的价值。一个属性越多的被用来在模型中构建决策树，它的重要性就相对越高。

属性重要性是通过对数据集中的每个属性进行计算，并进行排序得到。在单个决策树中通过每个属性分裂点改进性能度量的量来计算属性重要性。由节点负责加权和记录次数，也就是说一个属性对分裂点改进性能度量越大（越靠近根节点），权值越大；被越多提升树所选择，属性越重要。性能度量可以是选择分裂节点的Gini纯度，也可以是其他度量函数。

　　最终将一个属性在所有提升树中的结果进行加权求和后然后平均，得到重要性得分。
　　
　　一个已训练的Xgboost模型能够自动计算特征重要性，这些重要性得分可以通过成员变量feature_importances_得到。

```python
print(model.feature_importances_)
```

我们可以直接在条形图上绘制这些分数，以便获得数据集中每个特征的相对重要性的直观显示


