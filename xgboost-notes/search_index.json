[
["XGBoost-learning.html", "XGBoost-learing 序言 Chapter 1 XGBoost 1.1 定义 1.2 特点 1.3 正则化目标函数 1.4 XGBoost目标函数 1.5 贪心算法 1.6 总结 Chapter 2 XGBoost20问 2.1 简述XGBoost 2.2 XGBoost与GBDT的差异 2.3 XGBoost为何使用二阶泰勒展开 2.4 XGBoost为何可并行训练 2.5 XGBoost为何快 2.6 XGBoost防止过拟合的方法 2.7 XGBoost处理缺失值 2.8 XGBoost叶子结点权重计算过程 2.9 XGBoost一棵树停止生长的条件 2.10 RF和GBDT的区别 2.11 XGBoost如何处理不平衡数据 2.12 比较LR和GBDT 2.13 XGBoost中如何对树进行剪枝 2.14 XGBoost如何选择最佳分裂点 2.15 XGBoost的Scalable性如何体现 2.16 XGBoost如何评价特征的重要性 2.17 XGBooost参数调优的一般步骤 2.18 XGBoost模型如果过拟合了怎么解决 2.19 XGBoost和LightGBM的区别 Chapter 3 波士顿房价预测", " XGBoost-learing 高文欣 2020-02-27 序言 XGBoost学习笔记 原理推导+实例 Chapter 1 XGBoost 1.1 定义 a novel sparsity-aware algorithm for sparse data and weighted quantile sketch for approximate tree learning. (Chen and Guestrin 2016) provide insights on cache access patterns, data compression and sharding to build a scalable tree boosting system. By combining these insights, XGBoost scales beyond billions of examples using far fewer resources than existing systems 提供了一种有关缓存访问模式，数据压缩和分片，以构建可扩展的boosting tree。 因此，XGBoost使用比现有系统少得多的资源来扩展数十亿个示例。 1.2 特点 The scalability of XGBoost is due to several important systems and algorithmic optimizations. These innovations include: a novel tree learning algorithm is for handling sparse data; a theoretically justified weighted quantile sketch procedure enables handling instance weights in approximate tree learning. Parallel and distributed computing makes learning faster which enables quicker model exploration. More importantly, XGBoost exploits out-of-core computation and enables data scientists to process hundred millions of examples on a desktop. Finally, it is even more exciting to combine these techniques to make an end-to-end system that scales to even larger data with the least amount of cluster resources. 可以处理稀疏矩阵 合适的权重使得误差较小 并行计算更快 可以利用核外资源，更快的处理大规模的数据 可扩展性强 构成一个端到端系统，以最少的集群资源扩展到更大的数据 1.2.1 回归树综述 回归树也叫回归与分类树，也是一棵叶子结点具有权重的二叉树 分类的规则与决策树的规则一样 每个叶子结点都包含一个权重（分数） 优点 对数据输入范围不敏感，因此不需要归一化处理 能学习特征之间更高阶别的相互关系 可拓展性强 适用范围广 1.3 正则化目标函数 假设一个数据集有\\(n\\)个样本和\\(m\\)个特征，\\(\\mathcal{D}=\\left\\{\\left(\\mathbf{x}_{i}, y_{i}\\right)\\right\\}\\left(|\\mathcal{D}|=n, \\mathbf{x}_{i} \\in \\mathbb{R}^{m}, y_{i} \\in \\mathbb{R}\\right)\\)，一个集成树使用\\(k\\)个加法模型进行预测。 \\(\\hat{y}_{i}=\\sum_{k=1}^{K} f_{k}\\left(x_{i}\\right), f_{k} \\in \\mathcal{F}\\) (1) \\(\\hat{y}_{i}\\)表示第 i个样本的预测值 K 表示一共K棵树 其中\\(\\mathcal{F}=\\left\\{f(\\mathbf{x})=w_{q(\\mathbf{x})}\\right\\}\\left(q: \\mathbb{R}^{m} \\rightarrow T, w \\in \\mathbb{R}^{T}\\right)\\) ,\\(\\mathcal{F}\\)表示所有基分类器组成的函数空间，比如CART. \\(q\\)表示表示将样本映射到的每棵树上相应的叶子索引 \\(T\\)是叶子结点的数量 \\(f_{k}(x_i)\\)表示第i个样本样本第k棵树中落在叶子结点得到的权重值 假设我们boosting的基模型用决策树来实现，则一颗生成好的决策树，即结构确定，也就是说树的叶子结点其实是确定了的。假设这棵树的叶子结点有 \\(T\\) 片叶子，而每片叶子对应的值 \\(w \\in R^{T}\\) 。熟悉决策树的同学应该清楚，每一片叶子结点中样本的预测值都会是一样的，在分类问题中是某一类，在回归问题中，是某一个值（在GBDT中都是回归树，即分类问题转化成对概率的回归了），那么肯定存在这样一个函数\\(q:R^d-&gt;{1,2,...T}\\),即将 \\(f_{t}(x)\\) 中的每个样本映射到每一个叶子结点上，当然 \\(f_{t}(x)\\)和 q 我们都是不知道的，但我们也不关心，这里只是说明一下决策树表达数据结构的方法是怎么样的，不理解也没有问题。 下面来正式推导： \\(f_{t}(x)\\)可以转化为\\(w_{q(x)}\\),其中\\(q(x)\\) 代表了每个样本在哪个叶子结点上,而 \\(w_q\\) 则代表了哪个叶子结点取什么 \\(w\\) 值，所以 \\(w_{q(x)}\\) 就代表了每个样本的取值\\(w\\) (即预测值). 如果决策树的复杂度可以由正则项来定义 \\(\\Omega\\left(f_{t}\\right)=\\gamma T+\\frac{1}{2} \\lambda \\sum_{j=1}^{T} w_{j}^{2}\\) ，即决策树模型的复杂度由生成的树的叶子节点数量和叶子节点对应的值向量的L2范数决定。 在回归问题中，需要求出的就是\\(f_{k}\\),也就是每个树的结构和每个叶子结点的权重 1.4 XGBoost目标函数 \\(\\mathcal{L}(\\phi)=\\sum_{i} l\\left(\\hat{y}_{i}, y_{i}\\right)+\\sum_{k} \\Omega\\left(f_{k}\\right)\\) where \\(\\Omega(f)=\\gamma T+\\frac{1}{2} \\lambda\\|w\\|^{2}\\) (2) \\(\\sum_{i} l\\left(\\hat{y}_{i},y_{i}\\right)\\)是损失函数，也就是训练误差，换句话说就是训练出来的函数与测试集的匹配程度。 \\(\\sum_{k} \\Omega\\left(f_{k}\\right)\\)表示模型的复杂度，也就是分段的琐碎程度 加入正则惩罚项时为了防止过拟合 为什么说加入正则化可以防止过拟合 正则项时我们假设反应模型的复杂度的，因此模型应该是越简单越好，需要加入正则项进行控制 - L2范数Ridge：\\(\\Omega(w)=\\lambda\\|w\\|^{2}\\)，可以认为是摸的平方 - L1范数Lasso：\\(\\Omega(w)=\\lambda\\|w\\|_{1}\\) 1.4.1 常见的正则优化函数 岭回归Ridge：由方差和L2范数构成 \\(\\sum_{i=1}^{n}\\left(y_{i}-w^{T} x_{i}\\right)^{2}+\\lambda\\|w\\|^{2}\\) 套索回归Lasso：方差和L1范数构成 \\(\\sum_{i=1}^{n}\\left(y_{i}-w^{T} x_{i}\\right)^{2}+\\lambda\\|w\\|_{1}\\) logistics回归：logistics误差和L2范数构成，主要用于二分类问题，最常见 \\(\\sum_{i=1}^{n}\\left[y_{i} \\ln \\left(1+e^{-w^{T} x_{i}}\\right)+\\left(1-y_{i}\\right) \\ln \\left(1+e^{w^{T} x_{i}}\\right)\\right]+\\lambda\\|w\\|^{2}\\) 训练模型的目标就是得到最小化的损失函数，因此需要，\\(\\mathcal{L}(\\phi)\\)越小越好，那就是\\(\\sum_{i} l\\left(\\hat{y}_{i},y_{i}\\right)\\)和正则项都取小，但是根据trade-off思维，需要权衡bias和variance的大小 1.4.2 损失函数分类 若训练误差为：\\(l\\left(y, \\hat{y}_{i}\\right)=\\left(y_{i}-\\hat{y}_{i}\\right)^{2}\\)，此使就是GBDT 若训练误差为\\(l\\left(y, \\hat{y}_{i}\\right)=y_{i} \\ln \\left(1+e^{-\\hat{y}_{i}}+\\left(1-y_{i}\\right) \\ln \\left(1+e^{\\hat{y}_{i}}\\right)\\right)\\)那就是logistics 1.5 贪心算法 贪心算法的思想就是一棵树，一棵树的往上加，加到K棵树直到算法停止 对于Boosting来说，它采用的是前向优化算法，即从前往后，逐渐建立基模型来优化逼近目标函数，具体过程如下： \\[ \\begin{aligned} &amp;\\hat{y}_{i}^{0}=0\\\\ &amp;\\hat{y}_{i}^{1}=f_{1}\\left(x_{i}\\right)=\\hat{y}_{i}^{0}+f_{1}\\left(x_{i}\\right)\\\\ &amp;\\begin{array}{l} \\hat{y}_{i}^{2}=f_{1}\\left(x_{i}\\right)+f_{2}\\left(x_{i}\\right)=\\hat{y}_{i}^{1}+f_{2}\\left(x_{i}\\right) \\\\ \\therefore \\quad \\hat{y}_{i}^{t}=\\sum_{k=1}^{t} f_{k}\\left(x_{i}\\right)=\\hat{y}_{i}^{t-1}+f_{t}\\left(x_{i}\\right) \\end{array} \\end{aligned} \\] 其中\\(f_{k}\\)的加入，也就是新模型的加入总是以优化目标函数为目的的。 1.5.1 过程 以第t步的模型拟合为例，在这一步，模型对第 \\(i\\)个样本 \\(x_i\\) 的预测为： \\(\\hat{y}_{i}^{t}=\\hat{y}_{i}^{t-1}+f_{t}\\left(x_{i}\\right)\\) 其中 \\(f_{t}(x_{i})\\) 就是我们这次需要加入的新模型，即需要拟合的模型，此时，目标函数就可以写成： \\(\\begin{aligned} O b j^{(t)} &amp;=\\sum_{i=1}^{n} l\\left(y_{i}, \\hat{y}_{i}^{t}\\right)+\\sum_{i=i}^{t} \\Omega\\left(f_{i}\\right) \\\\ &amp;=\\sum_{i=1}^{n} l\\left(y_{i}, \\hat{y}_{i}^{t-1}+f_{t}\\left(x_{i}\\right)\\right)+\\Omega\\left(f_{t}\\right)+\\text { constant } \\end{aligned}\\) (3) 因此当求出最优目标函数的时候也就相当于求出了\\(f_{t}\\left(x_{i}\\right)\\) 我们知道泰勒公式中，若\\(\\Delta x\\) 很小时，我们只保留二阶导是合理的（GBDT是一阶导，XGBoost是二阶导，我们以二阶导为例，一阶导可以自己去推，因为更简单） 或许也可以说我们更希望将优化问题转化为一个凸优化问题，因此而引入二阶泰特展开式，即： \\(f(x+\\Delta x) \\approx f(x)+f^{\\prime}(x) \\Delta x+\\frac{1}{2} f^{\\prime \\prime}(x) \\Delta x^{2}\\) (4) 那么在等式（3）中，我们把 \\(\\hat{y}_{i}^{t-1}\\) 看成是等式（4）中的x， \\(f_{t}\\left(x_{i}\\right)\\) 看成是 \\(\\Delta x\\) ，因此等式（3）可以写成： \\(O b j^{(t)}=\\sum_{i=1}^{n}\\left[l\\left(y_{i}, \\hat{y}_{i}^{t-1}\\right)+g_{i} f_{t}\\left(x_{i}\\right)+\\frac{1}{2} h_{i} f_{t}^{2}\\left(x_{i}\\right)\\right]+\\Omega\\left(f_{t}\\right)+\\) constant (5) 其中 \\(g_{i}\\) 为损失函数的一阶导， \\(h_i\\) 为损失函数的二阶导，注意这里的导是对 \\(\\hat{y}_{i}^{t-1}\\) 求导。我们以 平方损失函数为例\\(\\sum_{i=1}^{n}\\left(y_{i}-\\left(\\hat{y}_{i}^{t-1}+f_{t}\\left(x_{i}\\right)\\right)\\right)^{2}\\) ，则分别给出\\(g_i\\),\\(h_i\\) \\(g_{i}=\\partial_{\\hat{y}^{t-1}}\\left(\\hat{y}^{t-1}-y_{i}\\right)^{2}=2\\left(\\hat{y}^{t-1}-y_{i}\\right), \\quad h_{i}=\\partial_{\\hat{y}^{t-1}}^{2}\\left(\\hat{y}^{t-1}-y_{i}\\right)^{2}=2\\) 由于在第t步 \\(\\hat{y}_{i}^{t-1}\\) 其实是一个已知的值，所以 \\(l\\left(y_{i}, \\hat{y}_{i}^{t-1}\\right)\\) 是一个常数，其对函数优化不会产生影响，因此，等式（3）可以写成： \\(O b j^{(t)} \\approx \\sum_{i=1}^{n}\\left[g_{i} f_{t}\\left(x_{i}\\right)+\\frac{1}{2} h_{i} f_{t}^{2}\\left(x_{i}\\right)\\right]+\\Omega\\left(f_{t}\\right)\\) (6) 所以我么只要求出每一步损失函数的一阶和二阶导的值（由于前一步的 \\(\\hat{y}_{i}^{t-1}\\) 是已知的，所以这两个值就是常数）代入等式4，然后最优化目标函数，就可以得到每一步的 \\(f(x)\\) ，最后根据加法模型得到一个整体模型 1.5.2 如何使用决策树表示目标函数 假设我们boosting的基模型用决策树来实现，则一颗生成好的决策树，即结构确定，也就是说树的叶子结点其实是确定了的。假设这棵树的叶子结点有 \\(T\\) 片叶子，而每片叶子对应的值 \\(w \\in R^{T}\\) 。熟悉决策树的同学应该清楚，每一片叶子结点中样本的预测值都会是一样的，在分类问题中是某一类，在回归问题中，是某一个值（在GBDT中都是回归树，即分类问题转化成对概率的回归了），那么肯定存在这样一个函数\\(q:R^d-&gt;{1,2,...T}\\),即将 \\(f_{t}(x)\\) 中的每个样本映射到每一个叶子结点上，当然 \\(f_{t}(x)\\)和 q 我们都是不知道的，但我们也不关心，这里只是说明一下决策树表达数据结构的方法是怎么样的，不理解也没有问题。 下面来正式推导： \\(f_{t}(x)\\)可以转化为\\(w_{q(x)}\\),其中\\(q(x)\\) 代表了每个样本在哪个叶子结点上,而 \\(w_q\\) 则代表了哪个叶子结点取什么 \\(w\\) 值，所以 \\(w_{q(x)}\\) 就代表了每个样本的取值\\(w\\) (即预测值). 如果决策树的复杂度可以由正则项来定义 \\(\\Omega\\left(f_{t}\\right)=\\gamma T+\\frac{1}{2} \\lambda \\sum_{j=1}^{T} w_{j}^{2}\\) ，即决策树模型的复杂度由生成的树的叶子节点数量和叶子节点对应的值向量的L2范数决定。 我们假设 \\(I_{j}=\\left\\{i | q\\left(x_{i}\\right)=j\\right\\}\\) 为第 j 个叶子节点的样本集合，则等式4根据上面的一些变换可以写成： \\(\\begin{aligned} O b j^{(t)} &amp; \\approx \\sum_{i=1}^{n}\\left[g_{i} f_{t}\\left(x_{i}\\right)+\\frac{1}{2} h_{i} f_{t}^{2}\\left(x_{i}\\right)\\right]+\\Omega\\left(f_{t}\\right) \\\\ &amp;=\\sum_{i=1}^{n}\\left[g_{i} w_{q\\left(x_{i}\\right)}+\\frac{1}{2} h_{i} w_{q\\left(x_{i}\\right)}^{2}\\right]+\\gamma T+\\frac{1}{2} \\lambda \\sum_{j=1}^{T} w_{j}^{2} \\\\ &amp;=\\sum_{j=1}^{T}\\left[\\left(\\sum_{i \\in I_{j}} g_{i}\\right) w_{j}+\\frac{1}{2}\\left(\\sum_{i \\in I_{j}} h_{i}+\\lambda\\right) w_{j}^{2}\\right]+\\gamma T \\end{aligned}\\) (7) 即我们之前样本的集合，现在都改写成叶子结点的集合，由于一个叶子结点有多个样本存在，因此才有了\\(\\sum_{i \\in I_{j}} g_{i}\\)和\\(\\sum_{i \\in I_{j}} h_{i}\\)两 项，定义\\(G_i=\\sum_{i \\in I_{j}} g_{i}\\),\\(H_i=\\sum_{i \\in I_{j}} h_{i}\\),则公式7可以写成: \\(O b j^{(t)}=\\sum_{j=1}^{T}\\left[G_{j} w_{j}+\\frac{1}{2}\\left(H_{j}+\\lambda\\right) w_{j}^{2}\\right]+\\gamma T\\) 1.5.3 如何优化目标函数 那么对于单棵决策树，一种理想的优化状态就是枚举所有可能的树结构，因此过程如下： a、首先枚举所有可能的树结构，即 \\(q\\)； b、计算每种树结构下的目标函数值，即等式7的值； c、取目标函数最小（大）值为最佳的数结构，根据等式6求得每个叶子节点的 \\(w\\) 取值，即样本的预测值。 但上面的方法肯定是不可行的，因为树的结构千千万，所以一般用贪心策略来优化： a、从深度为0的树开始，对每个叶节点枚举所有的可用特征 b、 针对每个特征，把属于该节点的训练样本根据该特征值升序排列，通过线性扫描的方式来决定该特征的最佳分裂点，并记录该特征的最大收益（采用最佳分裂点时的收益） c、 选择收益最大的特征作为分裂特征，用该特征的最佳分裂点作为分裂位置，把该节点生长出左右两个新的叶节点，并为每个新节点关联对应的样本集 d、回到第1步，递归执行到满足特定条件为止 1.5.4 如何分裂一个结点 那么如何计算上面的收益呢，很简单，仍然紧扣目标函数就可以了。假设我们在某一节点上二分裂成两个节点，分别是左（L）右（R），则分列前的目标函数是: \\[-\\frac{1}{2}\\left[\\frac{\\left(G_{L}+G_{R}\\right)^{2}}{H_{L}+H_{R}+\\lambda}\\right]+\\gamma\\] 分裂后\\[-\\frac{1}{2}\\left[\\frac{G_{L}^{2}}{H_{L}+\\lambda}+\\frac{G_{R}^{2}}{H_{R}+\\lambda}\\right]+2\\gamma\\]，则对于目标函数来说，分裂后的收益是（这里假设是最小化目标函数，所以用分裂前-分裂后） \\[Gain =\\frac{1}{2}\\left[\\frac{G_{L}^{2}}{H_{L}+\\lambda}+\\frac{G_{R}^{2}}{H_{R}+\\lambda}-\\frac{\\left(G_{L}+G_{R}\\right)^{2}}{H_{L}+H_{R}+\\lambda}\\right]-\\gamma\\] 如果增益Gain&gt;0，即分裂为两个叶子节点后，目标函数下降了，那么我们会考虑此次分裂的结果。 但是，在一个结点分裂时，可能有很多个分裂点，每个分裂点都会产生一个增益，如何才能寻找到最优的分裂点呢？接下来会讲到。 1.5.5 寻找最佳分裂点 在分裂一个结点时，我们会有很多个候选分割点，寻找最佳分割点的大致步骤如下： 遍历每个结点的每个特征； 对每个特征，按特征值大小将特征值排序； 线性扫描，找出每个特征的最佳分裂特征值； 在所有特征中找出最好的分裂点（分裂后增益最大的特征及特征值） 上面是一种贪心的方法，每次进行分裂尝试都要遍历一遍全部候选分割点，也叫做全局扫描法。 但当数据量过大导致内存无法一次载入或者在分布式情况下，贪心算法的效率就会变得很低，全局扫描法不再适用。 特征预排序+缓存：XGBoost在训练之前，预先对每个特征按照特征值大小进行排序，然后保存为block结构，后面的迭代中会重复地使用这个结构，使计算量大大减小。 分位点近似法：对每个特征按照特征值排序后，采用类似分位点选取的方式，仅仅选出常数个特征值作为该特征的候选分割点，在寻找该特征的最佳分割点时，从候选分割点中选出最优的一个。 并行查找：由于各个特性已预先存储为block结构，XGBoost支持利用多个线程并行地计算每个特征的最佳分割点，这不仅大大提升了结点的分裂速度，也极利于大规模训练集的适应性扩展。 1.5.6 停止生长 一棵树不会一直生长下去，下面是一些常见的限制条件。 当新引入的一次分裂所带来的增益Gain&lt;0时，放弃当前的分裂。这是训练损失和模型结构复杂度的博弈过程。 当树达到最大深度时，停止建树，因为树的深度太深容易出现过拟合，这里需要设置一个超参数max_depth。 当引入一次分裂后，重新计算新生成的左、右两个叶子结点的样本权重和。如果任一个叶子结点的样本权重低于某一个阈值，也会放弃此次分裂。这涉及到一个超参数:最小样本权重和，是指如果一个叶子节点包含的样本数量太少也会放弃分裂，防止树分的太细，这也是过拟合的一种措施。 1.6 总结 a、算法在拟合的每一步都新生成一颗决策树； b、在拟合这棵树之前，需要计算损失函数在每个样本上的一阶导和二阶导，即 \\(g_i\\) 和 \\(h_i\\) ； c、通过上面的贪心策略生成一颗树，计算每个叶子结点的的 \\(G_j\\)和 \\(H_j\\) ，利用等式6计算预测值 \\(w\\) ； d、把新生成的决策树 \\(f_{t}(x)\\) 加入 \\(\\hat{y}_{i}^{t}=\\hat{y}_{i}^{t-1}+\\epsilon f_{t}\\left(x_{i}\\right)\\) ，其中\\(\\epsilon\\) 为学习率，主要为了抑制模型的过拟 Chapter 2 XGBoost20问 2.1 简述XGBoost 首先需要说一说GBDT，它是一种基于boosting(串行)增强策略的加法模型，训练的时候采用前向分布算法进行贪婪的学习，每次迭代都学习一棵CART树来拟合之前 t-1 棵树的预测结果与训练样本真实值的残差。 XGBoost对GBDT进行了一系列优化，比如损失函数进行了二阶泰勒展开、目标函数加入正则项、支持并行和默认缺失值处理等，在可扩展性和训练速度上有了巨大的提升，但其核心思想没有大的变化 2.2 XGBoost与GBDT的差异 基分类器：XGBoost的基分类器不仅支持CART决策树，还支持线性分类器，在支持线性分类器的时候XGBoost相当于加入L1和L2的logistic回归(分类)，或者线性回归(回归)。 损失函数导数：XGBoost对损失函数做了二阶泰勒展开式，GBDT只是一阶泰勒，，并且，XGBoost还支持自定义损失函数，只要损失函数一阶，二阶可导。 正则项：XGBoost的目标函数加了正则项惩罚，相当于预剪枝，使学习出来的模型更加不容易过拟合，泛化能力相对更强。 列抽样：列抽样个人理解就是随机属性取样，与随机森林相似，可防止过拟合。 缺失值处理：对树中的每个非叶子结点，XGBoost可以自动学习出它默认的分裂方向，若某个仰恩该特征缺失，会将其划入默认分支。 并行化：不是tree的维度并行，而是特征维度并行。XGBoost预先将每个特征按特征值排好顺序，存储为块结构，分裂结点时可以采用线性并行查找每个特征的最佳分割点，极大提升训练速度。 2.3 XGBoost为何使用二阶泰勒展开 准确性：相对于GBDT一阶泰勒展开，XGBoost采用二阶泰勒展开精度更加靠近真实的损失函数。 可扩展性：损失函数支持自定义，只需要新的损失函数二阶可导就行。 2.4 XGBoost为何可并行训练 XGBoost的并行并不是每棵树都可以并行训练，XGB本质上仍然采用boosting思维，每棵树训练前需要等前面的树训练完成才能开始训练。也就是加法思维 XGBoost的并行是指特征维度的并行，在训练之前，每个特征按特征值对样本进行预排序，并存储为block结构，在后面查找特征分割点时可以重复使用，而且特征值已经被存储为一个个block结构，那么在寻找每个特征的最佳分割点时，可以利用多线程对每个block进行计算。 2.5 XGBoost为何快 分块并行：训练前每个特征按特征值进行排序并存储为Block结构，后面查找特征分割点时重复使用，并且支持并行查找每个特征的分割点 候选分位点：每个特征采用常数个分位点作为候选分割点 CPU cache 命中优化：使用缓存预取的方法，对每个线程分配一个连续的buffer，读取每个block中样本的梯度信息并存入连续的Buffer中。 Block 处理优化：Block预先放入内存；Block按列进行解压缩；将Block划分到不同硬盘来提高吞吐 2.6 XGBoost防止过拟合的方法 加入正则项：叶子结点的个数+叶子结点的权重 属性抽样，训练时只选取一部分特征，不考虑block剩余部分 子采样：每轮计算可以不使用全部样本，使算法更加保守 shrinkage: 可以叫学习率或步长，为了给后面的训练留出更多的学习空间 2.7 XGBoost处理缺失值 模型的一个优点就是允许特征存在缺失值。对缺失值的处理方式如下： 在特征k上寻找最佳 split point 时，不会对该列特征 missing 的样本进行遍历，而只对该列特征值为 non-missing 的样本上对应的特征值进行遍历，通过这个技巧来减少了为稀疏离散特征寻找 split point 的时间开销。 在逻辑实现上，为了保证完备性，会将该特征值missing的样本分别分配到左叶子结点和右叶子结点，两种情形都计算一遍后，选择分裂后增益最大的那个方向（左分支或是右分支），作为预测时特征值缺失样本的默认分支方向。 如果在训练中没有缺失值而在预测中出现缺失，那么会自动将缺失值的划分方向放到右子结点。 2.8 XGBoost叶子结点权重计算过程 2.9 XGBoost一棵树停止生长的条件 当新引入的一次分裂所带来的增益Gain&lt;0时，放弃当前的分裂。这是训练损失和模型结构复杂度的博弈过程。 当树达到最大深度时，停止建树，因为树的深度太深容易出现过拟合，这里需要设置一个超参数max_depth。 当引入一次分裂后，重新计算新生成的左、右两个叶子结点的样本权重和。如果任一个叶子结点的样本权重低于某一个阈值，也会放弃此次分裂。这涉及到一个超参数:最小样本权重和，是指如果一个叶子节点包含的样本数量太少也会放弃分裂，防止树分的太细。 2.10 RF和GBDT的区别 2.10.1 相同 都是由多棵树组成，最终的结果都是由多棵树一起决定。 2.10.2 不同 集成学习：RF属于bagging思想，而GBDT是boosting思想 偏差-方差权衡：RF不断的降低模型的方差，而GBDT不断的降低模型的偏差 训练样本：RF每次迭代的样本是从全部训练集中有放回抽样形成的，而GBDT每次使用全部样本 并行性：RF的树可以并行生成，而GBDT只能顺序生成(需要等上一棵树完全生成) 最终结果：RF最终是多棵树进行多数表决（回归问题是取平均），而GBDT是加权融合 数据敏感性：RF对异常值不敏感，而GBDT对异常值比较敏感 泛化能力：RF不易过拟合，而GBDT容易过拟合 2.11 XGBoost如何处理不平衡数据 对于不平衡的数据集，例如用户的购买行为，肯定是极其不平衡的，这对XGBoost的训练有很大的影响，XGBoost有两种自带的方法来解决： 第一种，如果你在意AUC，采用AUC来评估模型的性能，那你可以通过设置scale_pos_weight来平衡正样本和负样本的权重。例如，当正负样本比例为1:10时，scale_pos_weight可以取10； 第二种，如果你在意概率(预测得分的合理性)，你不能重新平衡数据集(会破坏数据的真实分布)，应该设置max_delta_step为一个有限数字来帮助收敛（基模型为LR时有效）。 原话是这么说的： &gt;For common cases such as ads clickthrough log, the dataset is extremely imbalanced. This can affect the training of xgboost model, and there are two ways to improve it. If you care only about the ranking order (AUC) of your prediction Balance the positive and negative weights, via scale_pos_weight Use AUC for evaluation If you care about predicting the right probability In such a case, you cannot re-balance the dataset In such a case, set parameter max_delta_step to a finite number (say 1) will help convergence 那么，源码到底是怎么利用scale_pos_weight来平衡样本的呢，是调节权重还是过采样呢？请看源码： if (info.labels[i] == 1.0f) w *= param_.scale_pos_weight 可以看出，应该是增大了少数样本的权重。 除此之外，还可以通过上采样、下采样、SMOTE算法或者自定义代价函数的方式解决正负样本不平衡的问题。 2.12 比较LR和GBDT 先说说LR和GBDT的区别： LR是线性模型，可解释性强，很容易并行化，但学习能力有限，需要大量的人工特征工程 GBDT是非线性模型，具有天然的特征组合优势，特征表达能力强，但是树与树之间无法并行训练，而且树模型很容易过拟合； 当在高维稀疏特征的场景下，LR的效果一般会比GBDT好。原因如下： 先看一个例子： 假设一个二分类问题，label为0和1，特征有100维，如果有1w个样本，但其中只要10个正样本1，而这些样本的特征 f1的值为全为1，而其余9990条样本的f1特征都为0(在高维稀疏的情况下这种情况很常见)。 我们都知道在这种情况下，树模型很容易优化出一个使用f1特征作为重要分裂节点的树，因为这个结点直接能够将训练数据划分的很好，但是当测试的时候，却会发现效果很差，因为这个特征f1只是刚好偶然间跟y拟合到了这个规律，这也是我们常说的过拟合。 那么这种情况下，如果采用LR的话，应该也会出现类似过拟合的情况呀：y = W1f1 + Wifi+….，其中 W1特别大以拟合这10个样本。为什么此时树模型就过拟合的更严重呢？ 仔细想想发现，因为现在的模型普遍都会带着正则项，而 LR 等线性模型的正则项是对权重的惩罚，也就是 W1一旦过大，惩罚就会很大，进一步压缩 W1的值，使他不至于过大。但是，树模型则不一样，树模型的惩罚项通常为叶子节点数和深度等，而我们都知道，对于上面这种 case，树只需要一个节点就可以完美分割9990和10个样本，一个结点，最终产生的惩罚项极其之小。 这也就是为什么在高维稀疏特征的时候，线性模型会比非线性模型好的原因了：带正则化的线性模型比较不容易对稀疏特征过拟合。 2.13 XGBoost中如何对树进行剪枝 在目标函数中增加了正则项：使用叶子结点的数目和叶子结点权重的L2模的平方，控制树的复杂度。 在结点分裂时，定义了一个阈值，如果分裂后目标函数的增益小于该阈值，则不分裂。 当引入一次分裂后，重新计算新生成的左、右两个叶子结点的样本权重和。如果任一个叶子结点的样本权重低于某一个阈值（最小样本权重和），也会放弃此次分裂。 XGBoost 先从顶到底建立树直到最大深度，再从底到顶反向检查是否有不满足分裂条件的结点，进行剪枝。 2.14 XGBoost如何选择最佳分裂点 XGBoost在训练前预先将特征按照特征值进行了排序，并存储为block结构，以后在结点分裂时可以重复使用该结构。 因此，可以采用特征并行的方法利用多个线程分别计算每个特征的最佳分割点，根据每次分裂后产生的增益，最终选择增益最大的那个特征的特征值作为最佳分裂点。 如果在计算每个特征的最佳分割点时，对每个样本都进行遍历，计算复杂度会很大，这种全局扫描的方法并不适用大数据的场景。XGBoost还提供了一种直方图近似算法，对特征排序后仅选择常数个候选分裂位置作为候选分裂点，极大提升了结点分裂时的计算效率。 2.15 XGBoost的Scalable性如何体现 基分类器的scalability：弱分类器可以支持CART决策树，也可以支持LR和Linear。 目标函数的scalability：支持自定义loss function，只需要其一阶、二阶可导。有这个特性是因为泰勒二阶展开，得到通用的目标函数形式。 学习方法的scalability：Block结构支持并行化，支持 Out-of-core计算。 2.16 XGBoost如何评价特征的重要性 我们采用三种方法来评判XGBoost模型中特征的重要程度： 官方文档： （1）weight - the number of times a feature is used to split the data across all trees. （2）gain - the average gain of the feature when it is used in trees. （3）cover - the average coverage of the feature when it is used in trees. weight ：该特征在所有树中被用作分割样本的特征的总次数。 gain ：该特征在其出现过的所有树中产生的平均增益。 cover ：该特征在其出现过的所有树中的平均覆盖范围。 注意：覆盖范围这里指的是一个特征用作分割点后，其影响的样本数量，即有多少样本经过该特征分割到两个子节点。 2.17 XGBooost参数调优的一般步骤 首先需要初始化一些基本变量，例如： max_depth = 5 min_child_weight = 1 gamma = 0 subsample, colsample_bytree = 0.8 scale_pos_weight = 1 确定learning rate和estimator的数量 learning rate可以先用0.1，用cv来寻找最优的estimators max_depth和 min_child_weight 我们调整这两个参数是因为，这两个参数对输出结果的影响很大。我们首先将这两个参数设置为较大的数，然后通过迭代的方式不断修正，缩小范围。 max_depth，每棵子树的最大深度，check from range(3,10,2)。 min_child_weight，子节点的权重阈值，check from range(1,6,2)。 如果一个结点分裂后，它的所有子节点的权重之和都大于该阈值，该叶子节点才可以划分。 gamma 也称作最小划分损失min_split_loss，check from 0.1 to 0.5，指的是，对于一个叶子节点，当对它采取划分之后，损失函数的降低值的阈值。 如果大于该阈值，则该叶子节点值得继续划分 如果小于该阈值，则该叶子节点不值得继续划分 subsample, colsample_bytree subsample是对训练的采样比例 colsample_bytree是对特征的采样比例 both check from 0.6 to 0.9 正则化参数 alpha 是L1正则化系数，try 1e-5, 1e-2, 0.1, 1, 100 lambda 是L2正则化系数 降低学习率 降低学习率的同时增加树的数量，通常最后设置学习率为0.01~0.1 2.18 XGBoost模型如果过拟合了怎么解决 当出现过拟合时，有两类参数可以缓解： - 第一类参数：用于直接控制模型的复杂度。包括max_depth,min_child_weight,gamma - 等参数 第二类参数：用于增加随机性，从而使得模型在训练时对于噪音不敏感。包括subsample,colsample_bytree - 还有就是直接减小learning rate，但需要同时增加estimator 参数。 ## 为什么XGBoost相比某些模型对缺失值不敏感 对存在缺失值的特征，一般的解决方法是： 离散型变量：用出现次数最多的特征值填充； 连续型变量：用中位数或均值填充； 一些模型如SVM和KNN，其模型原理中涉及到了对样本距离的度量，如果缺失值处理不当，最终会导致模型预测效果很差。 而树模型对缺失值的敏感度低，大部分时候可以在数据缺失时时使用。原因就是，一棵树中每个结点在分裂时，寻找的是某个特征的最佳分裂点（特征值），完全可以不考虑存在特征值缺失的样本，也就是说，如果某些样本缺失的特征值缺失，对寻找最佳分割点的影响不是很大。 XGBoost对缺失数据有特定的处理方法，详情参考上篇文章第7题。 因此，对于有缺失值的数据在经过缺失处理后： - 当数据量很小时，优先用朴素贝叶斯 - 数据量适中或者较大，用树模型，优先XGBoost - 数据量较大，也可以用神经网络 - 避免使用距离度量相关的模型，如KNN和SVM 2.19 XGBoost和LightGBM的区别 （1）树生长策略：XGB采用level-wise的分裂策略，LGB采用leaf-wise的分裂策略。XGB对每一层所有节点做无差别分裂，但是可能有些节点增益非常小，对结果影响不大，带来不必要的开销。Leaf-wise是在所有叶子节点中选取分裂收益最大的节点进行的，但是很容易出现过拟合问题，所以需要对最大深度做限制 。 （2）分割点查找算法：XGB使用特征预排序算法，LGB使用基于直方图的切分点算法，其优势如下： 减少内存占用，比如离散为256个bin时，只需要用8位整形就可以保存一个样本被映射为哪个bin(这个bin可以说就是转换后的特征)，对比预排序的exactgreedy算法来说(用int_32来存储索引+ 用float_32保存特征值)，可以节省7/8的空间。 计算效率提高，预排序的Exactgreedy对每个特征都需要遍历一遍数据，并计算增益，复杂度为𝑂(#𝑓𝑒𝑎𝑡𝑢𝑟𝑒×#𝑑𝑎𝑡𝑎)。而直方图算法在建立完直方图后，只需要对每个特征遍历直方图即可，复杂度为𝑂(𝑓𝑒𝑎𝑡𝑢𝑟𝑒 ×#𝑏𝑖𝑛𝑠)。 LGB还可以使用直方图做差加速，一个节点的直方图可以通过父节点的直方图减去兄弟节点的直方图得到，从而加速计算 &gt;但实际上xgboost的近似直方图算法也类似于lightgbm这里的直方图算法，为什么xgboost的近似算法比lightgbm还是慢很多呢？ xgboost在每一层都动态构建直方图， 因为xgboost的直方图算法不是针对某个特定的feature，而是所有feature共享一个直方图(每个样本的权重是二阶导)，所以每一层都要重新构建直方图，而lightgbm中对每个特征都有一个直方图，所以构建一次直方图就够了。 （3）支持离散变量：无法直接输入类别型变量，因此需要事先对类别型变量进行编码（例如独热编码），而LightGBM可以直接处理类别型变量。 （4）缓存命中率：XGB使用Block结构的一个缺点是取梯度的时候，是通过索引来获取的，而这些梯度的获取顺序是按照特征的大小顺序的，这将导致非连续的内存访问，可能使得CPU cache缓存命中率低，从而影响算法效率。而LGB是基于直方图分裂特征的，梯度信息都存储在一个个bin中，所以访问梯度是连续的，缓存命中率高。 （5）LightGBM 与 XGboost 的并行策略不同： 特征并行 ：LGB特征并行的前提是每个worker留有一份完整的数据集，但是每个worker仅在特征子集上进行最佳切分点的寻找；worker之间需要相互通信，通过比对损失来确定最佳切分点；然后将这个最佳切分点的位置进行全局广播，每个worker进行切分即可。XGB的特征并行与LGB的最大不同在于XGB每个worker节点中仅有部分的列数据，也就是垂直切分，每个worker寻找局部最佳切分点，worker之间相互通信，然后在具有最佳切分点的worker上进行节点分裂，再由这个节点广播一下被切分到左右节点的样本索引号，其他worker才能开始分裂。二者的区别就导致了LGB中worker间通信成本明显降低，只需通信一个特征分裂点即可，而XGB中要广播样本索引。 数据并行 ：当数据量很大，特征相对较少时，可采用数据并行策略。LGB中先对数据水平切分，每个worker上的数据先建立起局部的直方图，然后合并成全局的直方图，采用直方图相减的方式，先计算样本量少的节点的样本索引，然后直接相减得到另一子节点的样本索引，这个直方图算法使得worker间的通信成本降低一倍，因为只用通信以此样本量少的节点。XGB中的数据并行也是水平切分，然后单个worker建立局部直方图，再合并为全局，不同在于根据全局直方图进行各个worker上的节点分裂时会单独计算子节点的样本索引，因此效率贼慢，每个worker间的通信量也就变得很大。 投票并行（LGB）：当数据量和维度都很大时，选用投票并行，该方法是数据并行的一个改进。数据并行中的合并直方图的代价相对较大，尤其是当特征维度很大时。大致思想是：每个worker首先会找到本地的一些优秀的特征，然后进行全局投票，根据投票结果，选择top的特征进行直方图的合并，再寻求全局的最优分割点。 Chapter 3 波士顿房价预测 "]
]
