<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>processing</title>
  <meta name="description" content="processing" />
  <meta name="generator" content="bookdown 0.17 and GitBook 2.6.7" />

  <meta property="og:title" content="processing" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="processing" />
  
  
  

<meta name="author" content="高文欣" />


<meta name="date" content="2020-03-07" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  


<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />











<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(title);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path=""><a href="#序言"><i class="fa fa-check"></i>序言</a></li>
<li class="chapter" data-level="1" data-path=""><a href="#summary"><i class="fa fa-check"></i><b>1</b> summary</a><ul>
<li class="chapter" data-level="1.1" data-path=""><a href="#tidy-data"><i class="fa fa-check"></i><b>1.1</b> tidy data</a></li>
<li class="chapter" data-level="1.2" data-path=""><a href="#特征选择"><i class="fa fa-check"></i><b>1.2</b> 特征选择</a></li>
<li class="chapter" data-level="1.3" data-path=""><a href="#特征提取"><i class="fa fa-check"></i><b>1.3</b> 特征提取</a></li>
<li class="chapter" data-level="1.4" data-path=""><a href="#维数灾难"><i class="fa fa-check"></i><b>1.4</b> 维数灾难</a></li>
<li class="chapter" data-level="1.5" data-path=""><a href="#filter"><i class="fa fa-check"></i><b>1.5</b> filter</a><ul>
<li class="chapter" data-level="1.5.1" data-path=""><a href="#方差阈值特征选择"><i class="fa fa-check"></i><b>1.5.1</b> 方差阈值特征选择</a></li>
</ul></li>
<li class="chapter" data-level="1.6" data-path=""><a href="#特征中的缺失值的处理"><i class="fa fa-check"></i><b>1.6</b> 特征中的缺失值的处理</a></li>
<li class="chapter" data-level="1.7" data-path=""><a href="#pairwise-correlation"><i class="fa fa-check"></i><b>1.7</b> Pairwise correlation</a></li>
<li class="chapter" data-level="1.8" data-path=""><a href="#去掉相关性强的特征值"><i class="fa fa-check"></i><b>1.8</b> 去掉相关性强的特征值</a></li>
<li class="chapter" data-level="1.9" data-path=""><a href="#rfe"><i class="fa fa-check"></i><b>1.9</b> RFE</a></li>
<li class="chapter" data-level="1.10" data-path=""><a href="#基于树的特征选择"><i class="fa fa-check"></i><b>1.10</b> 基于树的特征选择</a></li>
<li class="chapter" data-level="1.11" data-path=""><a href="#正则化线性回归"><i class="fa fa-check"></i><b>1.11</b> 正则化线性回归</a></li>
<li class="chapter" data-level="1.12" data-path=""><a href="#集成器特征选择器"><i class="fa fa-check"></i><b>1.12</b> 集成器特征选择器</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path=""><a href="#pca"><i class="fa fa-check"></i><b>2</b> PCA</a><ul>
<li class="chapter" data-level="2.1" data-path=""><a href="#特征值分解协方差矩阵"><i class="fa fa-check"></i><b>2.1</b> 特征值分解协方差矩阵</a></li>
<li class="chapter" data-level="2.2" data-path=""><a href="#奇异值分解svd"><i class="fa fa-check"></i><b>2.2</b> 奇异值分解SVD</a></li>
<li class="chapter" data-level="2.3" data-path=""><a href="#应用"><i class="fa fa-check"></i><b>2.3</b> 应用</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path=""><a href="#t-sne"><i class="fa fa-check"></i><b>3</b> t-SNE</a><ul>
<li class="chapter" data-level="3.1" data-path=""><a href="#fit_transform"><i class="fa fa-check"></i><b>3.1</b> fit_transform</a></li>
<li class="chapter" data-level="3.2" data-path=""><a href="#简介"><i class="fa fa-check"></i><b>3.2</b> 简介</a></li>
<li class="chapter" data-level="3.3" data-path=""><a href="#demo"><i class="fa fa-check"></i><b>3.3</b> demo</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path=""><a href="#标准化data"><i class="fa fa-check"></i><b>4</b> 标准化data</a><ul>
<li class="chapter" data-level="4.1" data-path=""><a href="#最值标准化方法"><i class="fa fa-check"></i><b>4.1</b> 最值标准化方法</a></li>
<li class="chapter" data-level="4.2" data-path=""><a href="#均值方差标准化方法"><i class="fa fa-check"></i><b>4.2</b> 均值方差标准化方法</a></li>
<li class="chapter" data-level="4.3" data-path=""><a href="#scale"><i class="fa fa-check"></i><b>4.3</b> scale</a></li>
<li class="chapter" data-level="4.4" data-path=""><a href="#log归一化化数据"><i class="fa fa-check"></i><b>4.4</b> log归一化化数据</a></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">processing</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="header">
<h1 class="title">processing</h1>
<p class="author"><em>高文欣</em></p>
<p class="date"><em>2020-03-07</em></p>
</div>
<div id="序言" class="section level1 unnumbered">
<h1>序言</h1>
<ul>
<li>t-SNE</li>
<li>PCA</li>
<li>特征选择</li>
<li>特征提取</li>
</ul>

</div>
<div id="summary" class="section level1">
<h1><span class="header-section-number"> 1</span> summary</h1>
<p>再ML中“属性”==“特征”</p>
<div id="tidy-data" class="section level2">
<h2><span class="header-section-number">1.1</span> tidy data</h2>
<p>整齐数据</p>
<p>我们平时所使用的数据都是经过整理的整齐数据(TidyData)，然而实际上我们接收到的数据很多都是杂乱无章的，为了进行数据的预处理，我们需要先把数据转换为整齐的数据.</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb1-1" title="1">pd.shape</a></code></pre></div>
<p>查询混数据框的维度</p>
<p>一般可以通过seaborn绘制pairplot图，完后观察一下异常特征
对于不需要的特征可以采取的方式的是,删掉指定列</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb2-1" title="1">pd.drop(<span class="st">&quot;特征名&quot;</span>,axis<span class="op">=</span><span class="dv">1</span>)</a></code></pre></div>
</div>
<div id="特征选择" class="section level2">
<h2><span class="header-section-number">1.2</span> 特征选择</h2>
<ul>
<li>Filter：过滤法，按照发散性或者相关性对各个特征进行评分，设定阈值或者待选择阈值的个数，选择特征。</li>
<li>Wrapper：包装法，根据目标函数（通常是预测效果评分），每次选择若干特征，或者排除若干特征。</li>
<li>Embedded：嵌入法，先使用某些机器学习的算法和模型进行训练，得到各个特征的权值系数，根据系数从大到小选择特征。类似于Filter方法，但是是通过训练来确定特征的优劣。</li>
</ul>
<p>特征选择是从原始特征数据集中选择出子集，是一种包含的关系，没有更改原始的特征空间。</p>
<p>因为特征选择可以达到降维的目的，因此可以提高模型的泛化能力降低过拟合</p>
</div>
<div id="特征提取" class="section level2">
<h2><span class="header-section-number">1.3</span> 特征提取</h2>
<p>特征提取主要是通过属性间的关系，如组合不同的属性得到新的属性，这样就改变了原来的特征空间,新的特征可以是许多原有特征的线性组合。
&gt;extracted features can be quite hard to interpret</p>
<p>提取的特征可能很难解释，哈哈，是这样的吧，因为新特征是原有特征的线性组合</p>
<ul>
<li>PCA</li>
<li>LDA</li>
<li>SVD</li>
</ul>
</div>
<div id="维数灾难" class="section level2">
<h2><span class="header-section-number">1.4</span> 维数灾难</h2>
<p>特征过多，因此需要选择有意义的特征进入模型训练，因此选择降维</p>
</div>
<div id="filter" class="section level2">
<h2><span class="header-section-number">1.5</span> filter</h2>
<div id="方差阈值特征选择" class="section level3">
<h3><span class="header-section-number">1.5.1</span> 方差阈值特征选择</h3>
<p>也叫filter，就是移除方差低的
方差阈值（VarianceThreshold）</p>
<blockquote>
<p>是特征选择的一个简单方法，去掉那些方差没有达到阈值的特征。默认情况下，删除零方差的特征，例如那些只有一个值的样本。
假设我们有一个有布尔特征的数据集，然后我们想去掉那些超过80%的样本都是0（或者1）的特征。布尔特征是伯努利随机变量，方差为 p(1-p)。<a href="https://www.jianshu.com/p/b3056d10a20f">zhilaizhiwang</a></p>
</blockquote>
<p>也就是选择方差大于阈值的特征</p>
<p>可以通过boxplot观察方差大小
可以直接使用sklearn中的VarianceThreshold方法</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb3-1" title="1"><span class="im">from</span> sklearn.feature_selection <span class="im">import</span> VarianceThreshold</a>
<a class="sourceLine" id="cb3-2" title="2"></a>
<a class="sourceLine" id="cb3-3" title="3"><span class="co"># Create a VarianceThreshold feature selector</span></a>
<a class="sourceLine" id="cb3-4" title="4">sel <span class="op">=</span>VarianceThreshold(threshold<span class="op">=</span><span class="fl">0.001</span>)</a>
<a class="sourceLine" id="cb3-5" title="5"></a>
<a class="sourceLine" id="cb3-6" title="6"><span class="co"># Fit the selector to normalized head_df</span></a>
<a class="sourceLine" id="cb3-7" title="7">sel.fit(head_df <span class="op">/</span> head_df.mean())</a>
<a class="sourceLine" id="cb3-8" title="8"></a>
<a class="sourceLine" id="cb3-9" title="9"><span class="co"># Create a boolean mask</span></a>
<a class="sourceLine" id="cb3-10" title="10">mask <span class="op">=</span> sel.get_support()</a>
<a class="sourceLine" id="cb3-11" title="11"></a>
<a class="sourceLine" id="cb3-12" title="12"><span class="co"># Apply the mask to create a reduced dataframe</span></a>
<a class="sourceLine" id="cb3-13" title="13">reduced_df <span class="op">=</span> head_df.loc[:,mask]</a>
<a class="sourceLine" id="cb3-14" title="14"></a>
<a class="sourceLine" id="cb3-15" title="15"><span class="bu">print</span>(<span class="st">&quot;Dimensionality reduced from </span><span class="sc">{}</span><span class="st"> to </span><span class="sc">{}</span><span class="st">.&quot;</span>.<span class="bu">format</span>(head_df.shape[<span class="dv">1</span>], reduced_df.shape[<span class="dv">1</span>]))</a>
<a class="sourceLine" id="cb3-16" title="16"></a>
<a class="sourceLine" id="cb3-17" title="17"><span class="op">&lt;</span>script.py<span class="op">&gt;</span> output:</a>
<a class="sourceLine" id="cb3-18" title="18">    Dimensionality reduced <span class="im">from</span> <span class="dv">6</span> to <span class="fl">4.</span></a></code></pre></div>
</div>
</div>
<div id="特征中的缺失值的处理" class="section level2">
<h2><span class="header-section-number">1.6</span> 特征中的缺失值的处理</h2>
<p>先补充一个计算缺失率的方法</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb4-1" title="1">df.isna().<span class="bu">sum</span>()<span class="op">/</span><span class="bu">len</span>(school_df)<span class="op">&lt;</span>p</a></code></pre></div>
<p>增加p是筛选缺失率小于xxx的特征</p>
</div>
<div id="pairwise-correlation" class="section level2">
<h2><span class="header-section-number">1.7</span> Pairwise correlation</h2>
<p>两两的相关系数</p>
<p>计算相关系数可以通过.corr()</p>
<p>heat_map()可是查看热力图，也就是两两相关系数的图</p>
<p>datacamp上面的一个小的栗子</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb5-1" title="1"><span class="co"># Create the correlation matrix</span></a>
<a class="sourceLine" id="cb5-2" title="2">corr <span class="op">=</span> ansur_df.corr()</a>
<a class="sourceLine" id="cb5-3" title="3"></a>
<a class="sourceLine" id="cb5-4" title="4"><span class="co"># Generate a mask for the upper triangle </span></a>
<a class="sourceLine" id="cb5-5" title="5">mask <span class="op">=</span> np.triu(np.ones_like(corr, dtype<span class="op">=</span><span class="bu">bool</span>))</a>
<a class="sourceLine" id="cb5-6" title="6"></a>
<a class="sourceLine" id="cb5-7" title="7"><span class="co"># Add the mask to the heatmap</span></a>
<a class="sourceLine" id="cb5-8" title="8">sns.heatmap(corr, mask<span class="op">=</span>mask, cmap<span class="op">=</span>cmap, center<span class="op">=</span><span class="dv">0</span>, linewidths<span class="op">=</span><span class="dv">1</span>, annot<span class="op">=</span><span class="va">True</span>, fmt<span class="op">=</span><span class="st">&quot;.2f&quot;</span>)</a>
<a class="sourceLine" id="cb5-9" title="9">plt.show()</a></code></pre></div>
<div class="sourceCode" id="cb6"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb6-1" title="1">knitr<span class="op">::</span><span class="kw">include_graphics</span>(<span class="st">&quot;./figs/02.png&quot;</span>)</a></code></pre></div>
<p><img src="figs/02.png" width="292" /></p>
<p>np.triu()返回三角矩阵，确实，heatmap返回上三角或者下三角就行了</p>
</div>
<div id="去掉相关性强的特征值" class="section level2">
<h2><span class="header-section-number">1.8</span> 去掉相关性强的特征值</h2>
<p>person相关系数法，filter的一种，去掉相关性强的</p>
<p>np.ones_like 返回一个用1填充的跟输入 形状和类型 一致的数组</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb7-1" title="1"><span class="co"># Calculate the correlation matrix and take the absolute value</span></a>
<a class="sourceLine" id="cb7-2" title="2"><span class="co"># 计算相关系数</span></a>
<a class="sourceLine" id="cb7-3" title="3">corr_matrix <span class="op">=</span> ansur_df.corr().<span class="bu">abs</span>()</a>
<a class="sourceLine" id="cb7-4" title="4"></a>
<a class="sourceLine" id="cb7-5" title="5"><span class="co"># Create a True/False mask and apply it</span></a>
<a class="sourceLine" id="cb7-6" title="6">mask <span class="op">=</span> np.triu(np.ones_like(corr_matrix, dtype<span class="op">=</span><span class="bu">bool</span>))</a>
<a class="sourceLine" id="cb7-7" title="7">tri_df <span class="op">=</span> corr_matrix.mask(mask)</a>
<a class="sourceLine" id="cb7-8" title="8"></a>
<a class="sourceLine" id="cb7-9" title="9"><span class="co"># List column names of highly correlated features (r &gt; 0.95)</span></a>
<a class="sourceLine" id="cb7-10" title="10">to_drop <span class="op">=</span> [c <span class="cf">for</span> c <span class="kw">in</span> tri_df.columns <span class="cf">if</span> <span class="bu">any</span>(tri_df[c] <span class="op">&gt;</span> <span class="fl">0.95</span>)]</a>
<a class="sourceLine" id="cb7-11" title="11"></a>
<a class="sourceLine" id="cb7-12" title="12"><span class="co"># Drop the features in the to_drop list</span></a>
<a class="sourceLine" id="cb7-13" title="13">reduced_df <span class="op">=</span> ansur_df.drop(to_drop, axis<span class="op">=</span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb7-14" title="14"></a>
<a class="sourceLine" id="cb7-15" title="15"><span class="bu">print</span>(<span class="st">&quot;The reduced_df dataframe has </span><span class="sc">{}</span><span class="st"> columns&quot;</span>.<span class="bu">format</span>(reduced_df.shape[<span class="dv">1</span>]))</a>
<a class="sourceLine" id="cb7-16" title="16"></a>
<a class="sourceLine" id="cb7-17" title="17"><span class="op">&lt;</span>script.py<span class="op">&gt;</span> output:</a>
<a class="sourceLine" id="cb7-18" title="18">    The reduced_df dataframe has <span class="dv">88</span> columns</a></code></pre></div>
</div>
<div id="rfe" class="section level2">
<h2><span class="header-section-number">1.9</span> RFE</h2>
<p>wrapper</p>
<p>递归式特征消除</p>
<blockquote>
<p>递归特征消除的主要思想是反复的构建模型（如SVM或者回归模型）然后选出最好的（或者最差的）的特征（可以根据coef来选），把选出来的特征选择出来，然后在剩余的特征上重复这个过程，直到所有特征都遍历了。这个过程中特征被消除的次序就是特征的排序。因此，这是一种寻找最优特征子集的贪心算法.<a href="https://blog.csdn.net/a2099948768/article/details/82454135">csdn</a></p>
</blockquote>
<div class="sourceCode" id="cb8"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb8-1" title="1"><span class="kw">class</span> sklearn.feature_selection.RFE(estimator, n_features_to_select<span class="op">=</span><span class="va">None</span>, step<span class="op">=</span><span class="dv">1</span>, verbose<span class="op">=</span><span class="dv">0</span>)</a></code></pre></div>
<p>参数</p>
<ul>
<li><p>estimator:学习器</p></li>
<li><p>n_features_to_select:特征选择的个数</p></li>
<li><p>step：int or float,可选(default=1)如果大于等于1，step对应于迭代过程中每次移除的属性的数量（integer）。如果是（0.0，1.0），就对应于每次移除的特征的比例，四舍五入</p></li>
</ul>
<p>属性</p>
<ul>
<li><p>n_features_：The number of selected features.</p></li>
<li><p>support_：array of shape [n_features]
The mask of selected features.选择特征特征的bool型，优秀特征是true，不优秀的特征是false？</p></li>
<li><p>ranking_:array of shape [n_features]
The feature ranking, such that ranking_[i] corresponds to the ranking position of the i-th feature. Selected (i.e., estimated best) features are assigned rank 1.输出第i个特征的排名位置</p></li>
<li><p>estimator_：object
The external estimator fit on the reduced dataset.其他能减少数据集的估计器</p></li>
</ul>
<div class="sourceCode" id="cb9"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb9-1" title="1"> </a>
<a class="sourceLine" id="cb9-2" title="2"><span class="im">from</span> sklearn.feature_selection <span class="im">import</span> RFE</a>
<a class="sourceLine" id="cb9-3" title="3"><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LogisticRegression</a>
<a class="sourceLine" id="cb9-4" title="4"><span class="im">from</span> sklearn.datasets <span class="im">import</span> load_iris</a>
<a class="sourceLine" id="cb9-5" title="5"><span class="im">from</span> sklearn.feature_selection <span class="im">import</span> RFE</a>
<a class="sourceLine" id="cb9-6" title="6"> </a>
<a class="sourceLine" id="cb9-7" title="7">iris <span class="op">=</span> load_iris()</a>
<a class="sourceLine" id="cb9-8" title="8"><span class="co">#特征矩阵</span></a>
<a class="sourceLine" id="cb9-9" title="9"><span class="co"># print(a)</span></a>
<a class="sourceLine" id="cb9-10" title="10"><span class="bu">print</span>(iris.data)</a>
<a class="sourceLine" id="cb9-11" title="11"> </a>
<a class="sourceLine" id="cb9-12" title="12"><span class="co">#目标向量</span></a>
<a class="sourceLine" id="cb9-13" title="13"><span class="co"># print(iris.target)</span></a>
<a class="sourceLine" id="cb9-14" title="14"> </a>
<a class="sourceLine" id="cb9-15" title="15"><span class="co">#递归特征消除法，返回特征选择后的数据</span></a>
<a class="sourceLine" id="cb9-16" title="16"><span class="co">#参数estimator为基模型</span></a>
<a class="sourceLine" id="cb9-17" title="17"><span class="co">#参数n_features_to_select为选择的特征个数</span></a>
<a class="sourceLine" id="cb9-18" title="18"><span class="bu">print</span>(RFE(estimator<span class="op">=</span>LogisticRegression(), n_features_to_select<span class="op">=</span><span class="dv">2</span>).fit_transform(iris.data, iris.target))</a>
<a class="sourceLine" id="cb9-19" title="19"> </a></code></pre></div>
</div>
<div id="基于树的特征选择" class="section level2">
<h2><span class="header-section-number">1.10</span> 基于树的特征选择</h2>
<p>embedding</p>
<blockquote>
<p>基于树的预测模型（见 sklearn.tree 模块，森林见 sklearn.ensemble 模块）能够用来计算特征的重要程度</p>
</blockquote>
<div class="sourceCode" id="cb10"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb10-1" title="1"><span class="co"># Perform a 75% training and 25% test data split</span></a>
<a class="sourceLine" id="cb10-2" title="2">X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X, y, test_size<span class="op">=</span><span class="fl">0.25</span>, random_state<span class="op">=</span><span class="dv">0</span>)</a>
<a class="sourceLine" id="cb10-3" title="3"></a>
<a class="sourceLine" id="cb10-4" title="4"><span class="co"># Fit the random forest model to the training data</span></a>
<a class="sourceLine" id="cb10-5" title="5">rf <span class="op">=</span> RandomForestClassifier(random_state<span class="op">=</span><span class="dv">0</span>)</a>
<a class="sourceLine" id="cb10-6" title="6">rf.fit(X_train, y_train)</a>
<a class="sourceLine" id="cb10-7" title="7"></a>
<a class="sourceLine" id="cb10-8" title="8"><span class="co"># Calculate the test set accuracy</span></a>
<a class="sourceLine" id="cb10-9" title="9">acc <span class="op">=</span> accuracy_score(y_test, rf.predict(X_test))</a>
<a class="sourceLine" id="cb10-10" title="10"></a>
<a class="sourceLine" id="cb10-11" title="11"><span class="co"># Print the importances per feature</span></a>
<a class="sourceLine" id="cb10-12" title="12"><span class="bu">print</span>(<span class="bu">dict</span>(<span class="bu">zip</span>(X.columns, rf.feature_importances_.<span class="bu">round</span>(<span class="dv">2</span>))))</a>
<a class="sourceLine" id="cb10-13" title="13"></a>
<a class="sourceLine" id="cb10-14" title="14"><span class="co"># Print accuracy</span></a>
<a class="sourceLine" id="cb10-15" title="15"><span class="bu">print</span>(<span class="st">&quot;</span><span class="sc">{0:.1%}</span><span class="st"> accuracy on test set.&quot;</span>.<span class="bu">format</span>(acc)) </a>
<a class="sourceLine" id="cb10-16" title="16"></a>
<a class="sourceLine" id="cb10-17" title="17"><span class="op">&lt;</span>script.py<span class="op">&gt;</span> output:</a>
<a class="sourceLine" id="cb10-18" title="18">    {<span class="st">&#39;diastolic&#39;</span>: <span class="fl">0.08</span>, <span class="st">&#39;pregnant&#39;</span>: <span class="fl">0.09</span>, <span class="st">&#39;age&#39;</span>: <span class="fl">0.16</span>, <span class="st">&#39;insulin&#39;</span>: <span class="fl">0.13</span>, <span class="st">&#39;glucose&#39;</span>: <span class="fl">0.21</span>, <span class="st">&#39;family&#39;</span>: <span class="fl">0.12</span>, <span class="st">&#39;bmi&#39;</span>: <span class="fl">0.09</span>, <span class="st">&#39;triceps&#39;</span>: <span class="fl">0.11</span>}</a>
<a class="sourceLine" id="cb10-19" title="19">    <span class="fl">77.6</span><span class="op">%</span> accuracy on test <span class="bu">set</span>.</a></code></pre></div>
<ul>
<li>get_support 方法来查看哪些特征被选中，它会返回所选特征的布尔遮罩（mask）<a href="https://www.cnblogs.com/stevenlk/p/6543628.html">参考</a></li>
</ul>
</div>
<div id="正则化线性回归" class="section level2">
<h2><span class="header-section-number">1.11</span> 正则化线性回归</h2>
<ul>
<li>LassoCV</li>
<li>RidgeCV</li>
</ul>
<div class="sourceCode" id="cb11"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb11-1" title="1"><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LassoCV</a>
<a class="sourceLine" id="cb11-2" title="2"></a>
<a class="sourceLine" id="cb11-3" title="3"><span class="co"># Create and fit the LassoCV model on the training set</span></a>
<a class="sourceLine" id="cb11-4" title="4">lcv <span class="op">=</span> LassoCV()</a>
<a class="sourceLine" id="cb11-5" title="5">lcv.fit(X_train, y_train)</a>
<a class="sourceLine" id="cb11-6" title="6"><span class="bu">print</span>(<span class="st">&#39;Optimal alpha = </span><span class="sc">{0:.3f}</span><span class="st">&#39;</span>.<span class="bu">format</span>(lcv.alpha_))</a>
<a class="sourceLine" id="cb11-7" title="7"></a>
<a class="sourceLine" id="cb11-8" title="8"><span class="co"># Calculate R squared on the test set</span></a>
<a class="sourceLine" id="cb11-9" title="9">r_squared <span class="op">=</span> lcv.score(X_test, y_test)</a>
<a class="sourceLine" id="cb11-10" title="10"><span class="bu">print</span>(<span class="st">&#39;The model explains </span><span class="sc">{0:.1%}</span><span class="st"> of the test set variance&#39;</span>.<span class="bu">format</span>(r_squared))</a>
<a class="sourceLine" id="cb11-11" title="11"></a>
<a class="sourceLine" id="cb11-12" title="12"><span class="co"># Create a mask for coefficients not equal to zero</span></a>
<a class="sourceLine" id="cb11-13" title="13">lcv_mask <span class="op">=</span> lcv.coef_ <span class="op">!=</span> <span class="dv">0</span></a>
<a class="sourceLine" id="cb11-14" title="14"><span class="bu">print</span>(<span class="st">&#39;</span><span class="sc">{}</span><span class="st"> features out of </span><span class="sc">{}</span><span class="st"> selected&#39;</span>.<span class="bu">format</span>(<span class="bu">sum</span>(lcv_mask), <span class="bu">len</span>(lcv_mask)))</a>
<a class="sourceLine" id="cb11-15" title="15"></a>
<a class="sourceLine" id="cb11-16" title="16">Optimal alpha <span class="op">=</span> <span class="fl">0.089</span></a>
<a class="sourceLine" id="cb11-17" title="17">The model explains <span class="fl">88.2</span><span class="op">%</span> of the test <span class="bu">set</span> variance</a>
<a class="sourceLine" id="cb11-18" title="18"><span class="dv">26</span> features out of <span class="dv">32</span> selected</a></code></pre></div>
</div>
<div id="集成器特征选择器" class="section level2">
<h2><span class="header-section-number">1.12</span> 集成器特征选择器</h2>
<div class="sourceCode" id="cb12"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb12-1" title="1"><span class="im">from</span> sklearn.feature_selection <span class="im">import</span> RFE</a>
<a class="sourceLine" id="cb12-2" title="2"><span class="im">from</span> sklearn.ensemble <span class="im">import</span> GradientBoostingRegressor</a>
<a class="sourceLine" id="cb12-3" title="3"></a>
<a class="sourceLine" id="cb12-4" title="4"><span class="co"># Select 10 features with RFE on a GradientBoostingRegressor, drop 3 features on each step</span></a>
<a class="sourceLine" id="cb12-5" title="5">rfe_gb <span class="op">=</span> RFE(estimator<span class="op">=</span>GradientBoostingRegressor(), </a>
<a class="sourceLine" id="cb12-6" title="6">             n_features_to_select<span class="op">=</span><span class="dv">10</span>, step<span class="op">=</span><span class="dv">3</span>, verbose<span class="op">=</span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb12-7" title="7">rfe_gb.fit(X_train, y_train)</a>
<a class="sourceLine" id="cb12-8" title="8"></a>
<a class="sourceLine" id="cb12-9" title="9"><span class="co"># Calculate the R squared on the test set</span></a>
<a class="sourceLine" id="cb12-10" title="10">r_squared <span class="op">=</span> rfe_gb.score(rfe_gb,y_test)</a>
<a class="sourceLine" id="cb12-11" title="11"><span class="bu">print</span>(<span class="st">&#39;The model can explain </span><span class="sc">{0:.1%}</span><span class="st"> of the variance in the test set&#39;</span>.<span class="bu">format</span>(r_squared))</a>
<a class="sourceLine" id="cb12-12" title="12"></a>
<a class="sourceLine" id="cb12-13" title="13"><span class="op">&lt;</span>script.py<span class="op">&gt;</span> output:</a>
<a class="sourceLine" id="cb12-14" title="14">    Fitting estimator <span class="cf">with</span> <span class="dv">32</span> features.</a>
<a class="sourceLine" id="cb12-15" title="15">    Fitting estimator <span class="cf">with</span> <span class="dv">29</span> features.</a>
<a class="sourceLine" id="cb12-16" title="16">    Fitting estimator <span class="cf">with</span> <span class="dv">26</span> features.</a>
<a class="sourceLine" id="cb12-17" title="17">    Fitting estimator <span class="cf">with</span> <span class="dv">23</span> features.</a>
<a class="sourceLine" id="cb12-18" title="18">    Fitting estimator <span class="cf">with</span> <span class="dv">20</span> features.</a>
<a class="sourceLine" id="cb12-19" title="19">    Fitting estimator <span class="cf">with</span> <span class="dv">17</span> features.</a>
<a class="sourceLine" id="cb12-20" title="20">    Fitting estimator <span class="cf">with</span> <span class="dv">14</span> features.</a>
<a class="sourceLine" id="cb12-21" title="21">    Fitting estimator <span class="cf">with</span> <span class="dv">11</span> features.</a></code></pre></div>
<p>可以有一个集成器，也可以组合多个集成器，or学习器</p>

</div>
</div>
<div id="pca" class="section level1">
<h1><span class="header-section-number"> 2</span> PCA</h1>
<blockquote>
<p>PCA(Principal-Component-Analysis)，即主成分分析方法，是一种使用最广泛的数据降维算法。PCA的主要思想是将n维特征映射到k维上，这k维是全新的正交特征也被称为主成分，是在原有n维特征的基础上重新构造出来的k维特征.PCA的工作就是从原始的空间中顺序地找一组相互正交的坐标轴，新的坐标轴的选择与数据本身是密切相关的。其中，第一个新坐标轴选择是原始数据中方差最大的方向，第二个新坐标轴选取是与第一个坐标轴正交的平面中使得方差最大的，第三个轴是与第1,2个轴正交的平面中方差最大的。依次类推，可以得到n个这样的坐标轴。通过这种方式获得的新的坐标轴，我们发现，大部分方差都包含在前面k个坐标轴中，后面的坐标轴所含的方差几乎为0。于是，我们可以忽略余下的坐标轴，只保留前面k个含有绝大部分方差的坐标轴。事实上，这相当于只保留包含绝大部分方差的维度特征，而忽略包含方差几乎为0的特征维度，实现对数据特征的降维处理。<a href="https://blog.csdn.net/program_developer/article/details/80632779">Microstrong0305</a></p>
</blockquote>
<ul>
<li><p>通过计算数据矩阵的协方差矩阵，然后得到协方差矩阵的特征值特征向量，选择<strong>特征值最大</strong>(即方差最大)的k个特征所对应的特征向量组成的矩阵。这样就可以将数据矩阵转换到新的空间当中，实现数据特征的降维。</p></li>
<li><p>方差是衡量数据的离散程度的，因此方差越大数据离散程度越高，PCA的偏好假设数据在低维空间越分散越好。</p></li>
<li><p>正交平面:可以理解为相互垂直的平面</p></li>
</ul>
<p>因此需要搞明白一个问题，为啥特征值大的方差就大？</p>
<p><strong>协方差</strong></p>
<p>样本均值：
<span class="math inline">\(\bar{x}=\frac{1}{n} \sum_{i=1}^{N} x_{i}\)</span></p>
<p>样本方差：
<span class="math inline">\(S^{2}=\frac{1}{n-1} \sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}\)</span></p>
<p>协方差：
<span class="math inline">\(\begin{aligned} \operatorname{Cov}(X, Y) &amp;=E[(X-E(X))(Y-E(Y))] \\ &amp;=\frac{1}{n-1} \sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)\left(y_{i}-\bar{y}\right) \end{aligned}\)</span></p>
<ol style="list-style-type: decimal">
<li><p>方差的计算公式是针对一维特征，即针对同一特征不同样本的取值来进行计算得到；而协方差则必须要求至少满足二维特征；方差是协方差的特殊情况。</p></li>
<li><p>方差和协方差的除数是n-1,这是为了得到方差和协方差的无偏估计。协方差为正时，说明X和Y是正相关关系；协方差为负时，说明X和Y是负相关关系；协方差为0时，说明X和Y是相互独立。Cov(X,X)就是X的方差。当样本是n维数据时，它们的协方差实际上是协方差矩阵(对称方阵)。</p></li>
</ol>
<p>举个三维的栗子</p>
<p><span class="math inline">\(\operatorname{Cov}(X, Y, Z)=\left[\begin{array}{lll}\operatorname{Cov}(x, x) &amp; \operatorname{Cov}(x, y) &amp; \operatorname{Cov}(x, z) \\ \operatorname{Cov}(y, x) &amp; \operatorname{Cov}(y, y) &amp; \operatorname{Cov}(y, z) \\ \operatorname{Cov}(z, x) &amp; \operatorname{Cov}(z, y) &amp; \operatorname{Cov}(z, z)\end{array}\right]\)</span></p>
<p><strong>散度矩阵</strong></p>
<p><span class="math inline">\(S=\sum_{k=1}^{n}\left(x_{k}-m\right)\left(x_{k}-m\right)^{T}\)</span></p>
<p><span class="math inline">\(m\)</span>是样本均值，散度矩阵就是协方差矩阵乘以（总数据量-1）。因此它们的特征值和特征向量是一样的</p>
<div id="特征值分解协方差矩阵" class="section level2">
<h2><span class="header-section-number">2.1</span> 特征值分解协方差矩阵</h2>
<p><strong>特征值与特征向量</strong>
<span class="math inline">\(A v=\lambda v\)</span></p>
<p><span class="math inline">\(A\)</span>是特征矩阵
<span class="math inline">\(v\)</span>是特征向量
<span class="math inline">\(\lambda\)</span>是特征向量对应的特征值</p>
<p><strong>特征值分解矩阵</strong>
就是将矩阵A分解成为正交矩阵
<span class="math inline">\(A=Q \Sigma Q^{-1}\)</span><span class="math inline">\(=Q\left[\begin{array}{cccc}\lambda_{1} &amp; \cdots &amp; \cdots &amp; \cdots \\ \cdots &amp; \lambda_{2} &amp; \cdots &amp; \cdots \\ \cdots &amp; \cdots &amp; \ddots &amp; \cdots \\ \cdots &amp; \cdots &amp; \cdots &amp; \lambda_{m}\end{array}\right] Q^{T}\)</span>，
只要将特征向量正交化单位化就会得到一组正交向量</p>
<p>Q是A特征向量组成的矩阵，<span class="math inline">\(\Sigma\)</span>是对焦矩阵，是A的特征值组成</p>
</div>
<div id="奇异值分解svd" class="section level2">
<h2><span class="header-section-number">2.2</span> 奇异值分解SVD</h2>
<blockquote>
<p>，奇异值可以被看作成一个矩阵的代表值，或者说，奇异值能够代表这个矩阵的信息。当奇异值越大时，它代表的信息越多。因此，我们取前面若干个最大的奇异值，就可以基本上还原出数据本身<a href="https://www.cnblogs.com/endlesscoding/p/10033527.html">参考</a></p>
</blockquote>
<p>奇异值分解是一个能适用于任意矩阵的一种分解的方法，对于任意矩阵A总是存在一个奇异值分解：<span class="math inline">\(A=U \Sigma V^{T}\)</span></p>
<p>假设A是一个m<em>n的矩阵，那么得到的U是一个m</em>m的方阵，U里面的正交向量被称为左奇异向量。Σ是一个m<em>n的矩阵，Σ除了对角线其它元素都为0，对角线上的元素称为奇异值。<span class="math inline">\(V^{T}\)</span>是v的转置矩阵，是一个n</em>n的矩阵，它里面的正交向量被称为右奇异值向量。而且一般来讲，我们会将Σ上的值按从大到小的顺序排列<a href="https://blog.csdn.net/program_developer/article/details/80632779">Microstrong0305</a></p>
<p>样本量很大的时候很有效。实际上，scikit-learn的PCA算法的背后真正的实现就是用的SVD，而不是特征值分解。</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb13-1" title="1">knitr<span class="op">::</span><span class="kw">include_graphics</span>(<span class="st">&#39;./figs/03.png&#39;</span>)</a></code></pre></div>
<p><img src="figs/03.png" width="766" /></p>
<p>具体的理论推导过程可以参考<a href="https://zhuanlan.zhihu.com/p/47858230">知乎专栏</a></p>
<p>奇异值分解的一个很好的应用是图片压缩</p>
<p>栗子还是sklearn中写好的函数
<a href="https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA">sklearn</a></p>
<div class="sourceCode" id="cb14"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb14-1" title="1"><span class="co"># Create the scaler</span></a>
<a class="sourceLine" id="cb14-2" title="2">scaler <span class="op">=</span> StandardScaler()</a>
<a class="sourceLine" id="cb14-3" title="3">ansur_std <span class="op">=</span> scaler.fit_transform(ansur_df)</a>
<a class="sourceLine" id="cb14-4" title="4"></a>
<a class="sourceLine" id="cb14-5" title="5"><span class="co"># Create the PCA instance and fit and transform the data with pca</span></a>
<a class="sourceLine" id="cb14-6" title="6">pca <span class="op">=</span> PCA()</a>
<a class="sourceLine" id="cb14-7" title="7">pc <span class="op">=</span> pca.fit_transform(ansur_std)</a>
<a class="sourceLine" id="cb14-8" title="8">pc_df <span class="op">=</span> pd.DataFrame(pc, columns<span class="op">=</span>[<span class="st">&#39;PC 1&#39;</span>, <span class="st">&#39;PC 2&#39;</span>, <span class="st">&#39;PC 3&#39;</span>, <span class="st">&#39;PC 4&#39;</span>])</a>
<a class="sourceLine" id="cb14-9" title="9"></a>
<a class="sourceLine" id="cb14-10" title="10"><span class="co"># Create a pairplot of the principal component dataframe</span></a>
<a class="sourceLine" id="cb14-11" title="11">sns.pairplot(pc_df)</a>
<a class="sourceLine" id="cb14-12" title="12">plt.show()</a>
<a class="sourceLine" id="cb14-13" title="13"><span class="co"># Inspect the explained variance ratio per component</span></a>
<a class="sourceLine" id="cb14-14" title="14"><span class="bu">print</span>(pca.explained_variance_ratio_)</a>
<a class="sourceLine" id="cb14-15" title="15"></a>
<a class="sourceLine" id="cb14-16" title="16"><span class="op">&lt;</span>script.py<span class="op">&gt;</span> output:</a>
<a class="sourceLine" id="cb14-17" title="17">    [<span class="fl">0.61449404</span> <span class="fl">0.19893965</span> <span class="fl">0.06803095</span> <span class="fl">0.03770499</span> <span class="fl">0.03031502</span> <span class="fl">0.0171759</span></a>
<a class="sourceLine" id="cb14-18" title="18">     <span class="fl">0.01072762</span> <span class="fl">0.00656681</span> <span class="fl">0.00634743</span> <span class="fl">0.00436015</span> <span class="fl">0.0026586</span>  <span class="fl">0.00202617</span></a>
<a class="sourceLine" id="cb14-19" title="19">     <span class="fl">0.00065268</span>]</a>
<a class="sourceLine" id="cb14-20" title="20"></a>
<a class="sourceLine" id="cb14-21" title="21"><span class="co"># Print the cumulative sum of the explained variance ratio</span></a>
<a class="sourceLine" id="cb14-22" title="22"><span class="bu">print</span>(pca.explained_variance_ratio_.cumsum())</a>
<a class="sourceLine" id="cb14-23" title="23"></a>
<a class="sourceLine" id="cb14-24" title="24"><span class="op">&lt;</span>script.py<span class="op">&gt;</span> output:</a>
<a class="sourceLine" id="cb14-25" title="25">    [<span class="fl">0.61449404</span> <span class="fl">0.81343368</span> <span class="fl">0.88146463</span> <span class="fl">0.91916962</span> <span class="fl">0.94948464</span> <span class="fl">0.96666054</span></a>
<a class="sourceLine" id="cb14-26" title="26">     <span class="fl">0.97738816</span> <span class="fl">0.98395496</span> <span class="fl">0.99030239</span> <span class="fl">0.99466254</span> <span class="fl">0.99732115</span> <span class="fl">0.99934732</span></a>
<a class="sourceLine" id="cb14-27" title="27">     <span class="fl">1.</span>        ]</a></code></pre></div>
<div class="sourceCode" id="cb15"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb15-1" title="1">knitr<span class="op">::</span><span class="kw">include_graphics</span>(<span class="st">&#39;./figs/04.png&#39;</span>)</a></code></pre></div>
<p><img src="figs/04.png" width="306" /></p>
<p>主成分选择</p>
</div>
<div id="应用" class="section level2">
<h2><span class="header-section-number">2.3</span> 应用</h2>
<p>1.EDA中可以结合pipline使用
2.进行图片压缩</p>

</div>
</div>
<div id="t-sne" class="section level1">
<h1><span class="header-section-number"> 3</span> t-SNE</h1>
<p>高维数据可视化方法</p>
<blockquote>
<p>t-SNE is a great technique for visual exploration of high dimensional datasets.</p>
</blockquote>
<div id="fit_transform" class="section level2">
<h2><span class="header-section-number">3.1</span> fit_transform</h2>
<p>fit(x,y)传两个参数是有监督学习的算法，
fit(x)传一个参数是无监督学习的算法</p>
<ul>
<li><p>fit(): Method calculates the parameters μ and σ and saves them as internal objects.
解释：简单来说，就是求得训练集X的均值，方差，最大值，最小值,这些训练集X固有的属性。</p></li>
<li><p>transform(): Method using these calculated parameters apply the transformation to a particular dataset.
解释：在fit的基础上，进行标准化，降维，归一化等操作（看具体用的是哪个工具，如PCA，StandardScaler等）。</p></li>
<li><p>fit_transform(): joins the fit() and transform() method for transformation of dataset.
解释：fit_transform是fit和transform的组合，既包括了训练又包含了转换。
transform()和fit_transform()二者的功能都是对数据进行某种统一处理（比如标准化~N(0,1)，将数据缩放(映射)到某个固定区间，归一化，正则化等）</p></li>
<li><p>fit_transform(trainData)对部分数据先拟合fit，找到该part的整体指标，如均值、方差、最大值最小值等等（根据具体转换的目的），然后对该trainData进行转换transform，从而实现数据的标准化、归一化等等。<a href="https://www.cnblogs.com/keye/p/8875128.html">cnblogs</a></p></li>
</ul>
</div>
<div id="简介" class="section level2">
<h2><span class="header-section-number">3.2</span> 简介</h2>
<ul>
<li>t-分布领域嵌入算法</li>
<li>虽然主打非线性高维数据降维，但是很少用，因为</li>
<li>比较适合应用于可视化，测试模型的效果</li>
<li>保证在低维上数据的分布与原始特征空间分布的相似性高<a href="https://www.zhihu.com/question/52022955/answer/387753267">知乎</a></li>
</ul>
<p><strong>因此用来查看分类器的效果更加</strong></p>
</div>
<div id="demo" class="section level2">
<h2><span class="header-section-number">3.3</span> demo</h2>
<div class="sourceCode" id="cb16"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb16-1" title="1"><span class="co"># Import TSNE</span></a>
<a class="sourceLine" id="cb16-2" title="2"><span class="im">from</span> sklearn.manifold <span class="im">import</span> TSNE </a>
<a class="sourceLine" id="cb16-3" title="3"></a>
<a class="sourceLine" id="cb16-4" title="4"><span class="co"># Create a TSNE instance: model</span></a>
<a class="sourceLine" id="cb16-5" title="5">model <span class="op">=</span> TSNE(learning_rate<span class="op">=</span><span class="dv">200</span>)</a>
<a class="sourceLine" id="cb16-6" title="6"></a>
<a class="sourceLine" id="cb16-7" title="7"><span class="co"># Apply fit_transform to samples: tsne_features</span></a>
<a class="sourceLine" id="cb16-8" title="8">tsne_features <span class="op">=</span> model.fit_transform(samples)</a>
<a class="sourceLine" id="cb16-9" title="9"></a>
<a class="sourceLine" id="cb16-10" title="10"><span class="co"># Select the 0th feature: xs</span></a>
<a class="sourceLine" id="cb16-11" title="11">xs <span class="op">=</span> tsne_features[:,<span class="dv">0</span>]</a>
<a class="sourceLine" id="cb16-12" title="12"></a>
<a class="sourceLine" id="cb16-13" title="13"><span class="co"># Select the 1st feature: ys</span></a>
<a class="sourceLine" id="cb16-14" title="14">ys <span class="op">=</span> tsne_features[:,<span class="dv">1</span>]</a>
<a class="sourceLine" id="cb16-15" title="15"></a>
<a class="sourceLine" id="cb16-16" title="16"><span class="co"># Scatter plot, coloring by variety_numbers</span></a>
<a class="sourceLine" id="cb16-17" title="17">plt.scatter(xs,ys,c<span class="op">=</span>variety_numbers)</a>
<a class="sourceLine" id="cb16-18" title="18">plt.show()</a></code></pre></div>
<div class="sourceCode" id="cb17"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb17-1" title="1">knitr<span class="op">::</span><span class="kw">include_graphics</span>(<span class="st">&quot;./figs/01.png&quot;</span>)</a></code></pre></div>
<p><img src="figs/01.png" width="458" /></p>

</div>
</div>
<div id="标准化data" class="section level1">
<h1><span class="header-section-number"> 4</span> 标准化data</h1>
<p>特征数字差值很大的属性会对计算结果产生很大的影响，当我们认为特征是等权重的时候，因为取值范围不同，因此要进行归一化</p>
<table>
<thead>
<tr class="header">
<th>time</th>
<th>distance</th>
<th>weight</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1.2</td>
<td>5000</td>
<td>80</td>
</tr>
<tr class="even">
<td>1.6</td>
<td>6000</td>
<td>90</td>
</tr>
<tr class="odd">
<td>1.0</td>
<td>3000</td>
<td>50</td>
</tr>
</tbody>
</table>
<p>例如我们认为，time，distance，weight三个权重是一样的，在做特征分析的时候会明显发现distance对计算结果的影响是最大的。
因此，使用归一化的方法将数值处理到0~1的范围内</p>
<div id="最值标准化方法" class="section level2">
<h2><span class="header-section-number">4.1</span> 最值标准化方法</h2>
<p><span class="math inline">\(x_{new}\)</span>=(<span class="math inline">\(x\)</span>-<span class="math inline">\(x_{min}\)</span>)/(<span class="math inline">\(x_{max}\)</span>-<span class="math inline">\(x_{min}\)</span>)</p>
<div class="sourceCode" id="cb18"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb18-1" title="1">cle&lt;-<span class="cf">function</span>(df){</a>
<a class="sourceLine" id="cb18-2" title="2">    df_new&lt;-(df<span class="op">-</span><span class="kw">min</span>(df))<span class="op">/</span>(<span class="kw">max</span>(df)<span class="op">-</span><span class="kw">min</span>(df))</a>
<a class="sourceLine" id="cb18-3" title="3">    return df_new</a>
<a class="sourceLine" id="cb18-4" title="4">}</a></code></pre></div>
</div>
<div id="均值方差标准化方法" class="section level2">
<h2><span class="header-section-number">4.2</span> 均值方差标准化方法</h2>
<p><span class="math inline">\(x_{\text {scale}}=\frac{x-x_{\text {mean}}}{s}\)</span></p>
<div class="sourceCode" id="cb19"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb19-1" title="1">cle&lt;-<span class="cf">function</span>(df){</a>
<a class="sourceLine" id="cb19-2" title="2">    df_new&lt;-(df<span class="op">-</span><span class="kw">mean</span>(df))<span class="op">/</span><span class="kw">std</span>(df)</a>
<a class="sourceLine" id="cb19-3" title="3">    return df_new</a>
<a class="sourceLine" id="cb19-4" title="4">}</a></code></pre></div>
<p>python中提供了standardscaler类可以直接对np对象进行均值方差标准化
<a href="https://www.cnblogs.com/xuezou/p/9332763.html">可以参考</a></p>
</div>
<div id="scale" class="section level2">
<h2><span class="header-section-number">4.3</span> scale</h2>
<p>sklearn中常见标准化函数是StandardScaler</p>
<div class="sourceCode" id="cb20"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb20-1" title="1"><span class="co"># Import the necessary modules</span></a>
<a class="sourceLine" id="cb20-2" title="2"><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> StandardScaler</a>
<a class="sourceLine" id="cb20-3" title="3"><span class="im">from</span> sklearn.pipeline <span class="im">import</span> Pipeline</a>
<a class="sourceLine" id="cb20-4" title="4"></a>
<a class="sourceLine" id="cb20-5" title="5"><span class="co"># Setup the pipeline steps: steps</span></a>
<a class="sourceLine" id="cb20-6" title="6">steps <span class="op">=</span> [(<span class="st">&#39;scaler&#39;</span>, StandardScaler()),</a>
<a class="sourceLine" id="cb20-7" title="7">        (<span class="st">&#39;knn&#39;</span>, KNeighborsClassifier())]</a>
<a class="sourceLine" id="cb20-8" title="8">        </a>
<a class="sourceLine" id="cb20-9" title="9"><span class="co"># Create the pipeline: pipeline</span></a>
<a class="sourceLine" id="cb20-10" title="10">pipeline <span class="op">=</span> Pipeline(steps)</a>
<a class="sourceLine" id="cb20-11" title="11"></a>
<a class="sourceLine" id="cb20-12" title="12"><span class="co"># Create train and test sets</span></a>
<a class="sourceLine" id="cb20-13" title="13">X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X, y, test_size<span class="op">=</span><span class="fl">0.3</span>, random_state<span class="op">=</span><span class="dv">42</span>)</a>
<a class="sourceLine" id="cb20-14" title="14"></a>
<a class="sourceLine" id="cb20-15" title="15"><span class="co"># Fit the pipeline to the training set: knn_scaled</span></a>
<a class="sourceLine" id="cb20-16" title="16">knn_scaled <span class="op">=</span> pipeline.fit(X_train, y_train)</a>
<a class="sourceLine" id="cb20-17" title="17"></a>
<a class="sourceLine" id="cb20-18" title="18"><span class="co"># Instantiate and fit a k-NN classifier to the unscaled data</span></a>
<a class="sourceLine" id="cb20-19" title="19">knn_unscaled <span class="op">=</span> KNeighborsClassifier().fit(X_train, y_train)</a>
<a class="sourceLine" id="cb20-20" title="20"></a>
<a class="sourceLine" id="cb20-21" title="21"><span class="co"># Compute and print metrics</span></a>
<a class="sourceLine" id="cb20-22" title="22"><span class="bu">print</span>(<span class="st">&#39;Accuracy with Scaling: </span><span class="sc">{}</span><span class="st">&#39;</span>.<span class="bu">format</span>(knn_scaled.score(X_test, y_test)))</a>
<a class="sourceLine" id="cb20-23" title="23"><span class="bu">print</span>(<span class="st">&#39;Accuracy without Scaling: </span><span class="sc">{}</span><span class="st">&#39;</span>.<span class="bu">format</span>(knn_unscaled.score(X_test, y_test)))</a>
<a class="sourceLine" id="cb20-24" title="24"></a>
<a class="sourceLine" id="cb20-25" title="25"><span class="op">&lt;</span>script.py<span class="op">&gt;</span> output:</a>
<a class="sourceLine" id="cb20-26" title="26">    Accuracy <span class="cf">with</span> Scaling: <span class="fl">0.7700680272108843</span></a>
<a class="sourceLine" id="cb20-27" title="27">    Accuracy without Scaling: <span class="fl">0.6979591836734694</span></a></code></pre></div>
<p>很明显，标准化之后的数据的预测精度更高</p>
</div>
<div id="log归一化化数据" class="section level2">
<h2><span class="header-section-number">4.4</span> log归一化化数据</h2>
<p>例如在回归模型中，因为在样本中的某些特征方差非常大，导致其他特征不起作用，因此需要训练模型之前先标准化数据。也就是缩放数据进行功能比较。</p>
<div class="sourceCode" id="cb21"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb21-1" title="1">In [<span class="dv">1</span>]<span class="op">:</span><span class="st"> </span><span class="kw">wine.head</span>()</a>
<a class="sourceLine" id="cb21-2" title="2">Out[<span class="dv">1</span>]<span class="op">:</span><span class="st"> </span></a>
<a class="sourceLine" id="cb21-3" title="3"><span class="st">   </span>Type  Alcohol   ...     OD280<span class="op">/</span>OD315 of diluted wines  Proline</a>
<a class="sourceLine" id="cb21-4" title="4"><span class="dv">0</span>     <span class="dv">1</span>    <span class="fl">14.23</span>   ...                             <span class="fl">3.92</span>     <span class="dv">1065</span></a>
<a class="sourceLine" id="cb21-5" title="5"><span class="dv">1</span>     <span class="dv">1</span>    <span class="fl">13.20</span>   ...                             <span class="fl">3.40</span>     <span class="dv">1050</span></a>
<a class="sourceLine" id="cb21-6" title="6"><span class="dv">2</span>     <span class="dv">1</span>    <span class="fl">13.16</span>   ...                             <span class="fl">3.17</span>     <span class="dv">1185</span></a>
<a class="sourceLine" id="cb21-7" title="7"><span class="dv">3</span>     <span class="dv">1</span>    <span class="fl">14.37</span>   ...                             <span class="fl">3.45</span>     <span class="dv">1480</span></a>
<a class="sourceLine" id="cb21-8" title="8"><span class="dv">4</span>     <span class="dv">1</span>    <span class="fl">13.24</span>   ...                             <span class="fl">2.93</span>      <span class="dv">735</span></a>
<a class="sourceLine" id="cb21-9" title="9"></a>
<a class="sourceLine" id="cb21-10" title="10">[<span class="dv">5</span> rows x <span class="dv">14</span> columns]</a></code></pre></div>
<p>比如Proline这个变量的存在就会导致其他变量不起作用，因此需要进行归一化。
常见方法可以使用log</p>
<div class="sourceCode" id="cb22"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb22-1" title="1"><span class="co"># Print out the variance of the Proline column</span></a>
<a class="sourceLine" id="cb22-2" title="2"><span class="bu">print</span>(wine[<span class="st">&quot;Proline&quot;</span>].var())</a>
<a class="sourceLine" id="cb22-3" title="3"></a>
<a class="sourceLine" id="cb22-4" title="4"><span class="co"># Apply the log normalization function to the Proline column</span></a>
<a class="sourceLine" id="cb22-5" title="5">wine[<span class="st">&quot;Proline_log&quot;</span>] <span class="op">=</span> np.log(wine[<span class="st">&quot;Proline&quot;</span>])</a>
<a class="sourceLine" id="cb22-6" title="6"></a>
<a class="sourceLine" id="cb22-7" title="7"><span class="co"># Check the variance of the normalized Proline column</span></a>
<a class="sourceLine" id="cb22-8" title="8"><span class="bu">print</span>(wine[<span class="st">&quot;Proline_log&quot;</span>].var())</a>
<a class="sourceLine" id="cb22-9" title="9"><span class="op">&lt;</span>script.py<span class="op">&gt;</span> output:</a>
<a class="sourceLine" id="cb22-10" title="10">    <span class="fl">99166.71735542436</span></a>
<a class="sourceLine" id="cb22-11" title="11">    <span class="fl">0.17231366191842012</span></a></code></pre></div>

</div>
</div>
            </section>

          </div>
        </div>
      </div>


    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": false,
"twitter": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"],
"google": false,
"instapper": false
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toolbar": {
"position": "fixed",
"search": false
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
